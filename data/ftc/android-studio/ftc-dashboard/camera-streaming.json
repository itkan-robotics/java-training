{
  "title": "Camera Streaming",
  "sections": [
    {
      "type": "text",
      "title": "Camera Streaming Overview",
      "content": "FTCDashboard's camera streaming feature allows you to view live video feeds from your robot's cameras directly in the web interface. This is invaluable for autonomous navigation, game element detection, and remote monitoring during testing and competition."
    },
    {
      "type": "rules-box",
      "title": "Camera Streaming Benefits",
      "items": [
        "Live video feed from robot cameras",
        "Multiple camera support and switching",
        "Real-time vision processing visualization",
        "Remote monitoring and debugging",
        "Team collaboration and strategy planning",
        "Competition field analysis and documentation"
      ]
    },
    {
      "type": "text",
      "title": "Camera Integration",
      "content": "To stream camera feeds to the dashboard, you need to integrate your camera hardware with the dashboard's streaming system. This typically involves setting up the camera, configuring the stream, and sending frames to the dashboard."
    },
    {
      "type": "code",
      "title": "Basic Camera Streaming Setup",
      "content": "This example shows how to set up basic camera streaming with FTCDashboard:",
      "code": "import com.acmerobotics.dashboard.FtcDashboard;\nimport com.acmerobotics.dashboard.telemetry.TelemetryPacket;\nimport com.qualcomm.robotcore.hardware.HardwareMap;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.Webcam;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.stream.CameraStreamSource;\nimport org.firstinspires.ftc.robotcore.external.stream.CameraStreamServer;\nimport org.firstinspires.ftc.robotcore.internal.camera.calibration.CameraCalibration;\nimport org.firstinspires.ftc.vision.VisionPortal;\nimport org.firstinspires.ftc.vision.VisionProcessor;\nimport org.opencv.core.Mat;\n\npublic class BasicCameraStreamOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    \n    // Camera hardware\n    private Webcam webcam;\n    private VisionPortal visionPortal;\n    private CameraStreamServer streamServer;\n    \n    // Camera configuration\n    public static int CAMERA_WIDTH = 640;\n    public static int CAMERA_HEIGHT = 480;\n    public static int CAMERA_FPS = 30;\n    public static boolean ENABLE_STREAMING = true;\n    \n    @Override\n    public void init() {\n        dashboard = FtcDashboard.getInstance();\n        packet = new TelemetryPacket();\n        \n        // Initialize camera\n        initCamera();\n        \n        // Set up streaming if enabled\n        if (ENABLE_STREAMING) {\n            setupCameraStream();\n        }\n        \n        packet.put(\"Status\", \"Camera initialized\");\n        packet.put(\"Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n        packet.put(\"FPS\", CAMERA_FPS);\n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void initCamera() {\n        // Get camera from hardware map\n        WebcamName webcamName = hardwareMap.get(WebcamName.class, \"webcam\");\n        \n        // Create vision portal with camera\n        visionPortal = VisionPortal.easyCreateWithDefaults(webcamName, new BasicVisionProcessor());\n        \n        // Configure camera settings\n        visionPortal.setProcessorEnabled(new BasicVisionProcessor(), true);\n        \n        // Set camera resolution and FPS\n        visionPortal.setCameraResolution(new Size(CAMERA_WIDTH, CAMERA_HEIGHT));\n        visionPortal.setCameraFrameRate(CAMERA_FPS);\n    }\n    \n    private void setupCameraStream() {\n        // Create camera stream server\n        streamServer = CameraStreamServer.getInstance();\n        \n        // Register camera as stream source\n        streamServer.setSource(new CameraStreamSource() {\n            @Override\n            public Mat getFrame() {\n                // Get the latest frame from the camera\n                return visionPortal.getLatestFrame();\n            }\n            \n            @Override\n            public int getFrameWidth() {\n                return CAMERA_WIDTH;\n            }\n            \n            @Override\n            public int getFrameHeight() {\n                return CAMERA_HEIGHT;\n            }\n            \n            @Override\n            public int getFps() {\n                return CAMERA_FPS;\n            }\n        });\n        \n        // Start the stream server\n        streamServer.start();\n    }\n    \n    @Override\n    public void loop() {\n        // Update camera status\n        updateCameraStatus();\n        \n        // Normal robot control code here\n        // ...\n        \n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void updateCameraStatus() {\n        packet.put(\"Camera Status\", visionPortal.getCameraState().toString());\n        packet.put(\"Streaming Active\", ENABLE_STREAMING && streamServer != null);\n        packet.put(\"Frame Count\", visionPortal.getFrameCount());\n        packet.put(\"FPS Actual\", visionPortal.getFps());\n    }\n    \n    @Override\n    public void stop() {\n        // Clean up camera resources\n        if (visionPortal != null) {\n            visionPortal.close();\n        }\n        if (streamServer != null) {\n            streamServer.stop();\n        }\n    }\n    \n    // Basic vision processor for camera integration\n    private static class BasicVisionProcessor implements VisionProcessor {\n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize vision processor\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process camera frame (basic implementation)\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw on frame if needed\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Multiple Camera Support",
      "content": "Many robots use multiple cameras for different purposes - one for navigation, another for game element detection, etc. FTCDashboard supports multiple camera streams that can be switched between or displayed simultaneously."
    },
    {
      "type": "code",
      "title": "Multiple Camera Support Example",
      "content": "This example demonstrates how to set up and manage multiple camera streams:",
      "code": "import com.acmerobotics.dashboard.FtcDashboard;\nimport com.acmerobotics.dashboard.telemetry.TelemetryPacket;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.vision.VisionPortal;\nimport org.firstinspires.ftc.vision.VisionProcessor;\nimport org.opencv.core.Mat;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class MultiCameraOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    \n    // Multiple cameras\n    private VisionPortal frontCamera;\n    private VisionPortal rearCamera;\n    private VisionPortal sideCamera;\n    \n    // Camera configuration\n    public static String ACTIVE_CAMERA = \"FRONT\"; // FRONT, REAR, SIDE, ALL\n    public static int CAMERA_WIDTH = 640;\n    public static int CAMERA_HEIGHT = 480;\n    public static int CAMERA_FPS = 30;\n    \n    // Camera management\n    private Map<String, VisionPortal> cameras = new HashMap<>();\n    private String currentActiveCamera = \"FRONT\";\n    private boolean multiViewEnabled = false;\n    \n    @Override\n    public void init() {\n        dashboard = FtcDashboard.getInstance();\n        packet = new TelemetryPacket();\n        \n        // Initialize all cameras\n        initMultipleCameras();\n        \n        // Set up camera switching\n        setupCameraSwitching();\n        \n        packet.put(\"Status\", \"Multiple cameras initialized\");\n        packet.put(\"Active Camera\", currentActiveCamera);\n        packet.put(\"Multi-view\", multiViewEnabled);\n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void initMultipleCameras() {\n        try {\n            // Initialize front camera (navigation)\n            WebcamName frontWebcam = hardwareMap.get(WebcamName.class, \"front_webcam\");\n            frontCamera = VisionPortal.easyCreateWithDefaults(frontWebcam, new NavigationProcessor());\n            cameras.put(\"FRONT\", frontCamera);\n            \n            // Initialize rear camera (game elements)\n            WebcamName rearWebcam = hardwareMap.get(WebcamName.class, \"rear_webcam\");\n            rearCamera = VisionPortal.easyCreateWithDefaults(rearWebcam, new GameElementProcessor());\n            cameras.put(\"REAR\", rearCamera);\n            \n            // Initialize side camera (obstacle detection)\n            WebcamName sideWebcam = hardwareMap.get(WebcamName.class, \"side_webcam\");\n            sideCamera = VisionPortal.easyCreateWithDefaults(sideWebcam, new ObstacleProcessor());\n            cameras.put(\"SIDE\", sideCamera);\n            \n            // Configure all cameras\n            for (VisionPortal camera : cameras.values()) {\n                camera.setCameraResolution(new Size(CAMERA_WIDTH, CAMERA_HEIGHT));\n                camera.setCameraFrameRate(CAMERA_FPS);\n            }\n            \n        } catch (Exception e) {\n            telemetry.addData(\"Camera Error\", \"Failed to initialize cameras: \" + e.getMessage());\n        }\n    }\n    \n    private void setupCameraSwitching() {\n        // Set up camera stream server for switching\n        CameraStreamServer streamServer = CameraStreamServer.getInstance();\n        \n        // Create stream source that can switch between cameras\n        streamServer.setSource(new CameraStreamSource() {\n            @Override\n            public Mat getFrame() {\n                if (multiViewEnabled) {\n                    // Return combined view of all cameras\n                    return createMultiViewFrame();\n                } else {\n                    // Return frame from active camera\n                    VisionPortal activeCamera = cameras.get(currentActiveCamera);\n                    return activeCamera != null ? activeCamera.getLatestFrame() : null;\n                }\n            }\n            \n            @Override\n            public int getFrameWidth() {\n                return multiViewEnabled ? CAMERA_WIDTH * 2 : CAMERA_WIDTH;\n            }\n            \n            @Override\n            public int getFrameHeight() {\n                return multiViewEnabled ? CAMERA_HEIGHT * 2 : CAMERA_HEIGHT;\n            }\n            \n            @Override\n            public int getFps() {\n                return CAMERA_FPS;\n            }\n        });\n        \n        streamServer.start();\n    }\n    \n    private Mat createMultiViewFrame() {\n        // Create a combined frame showing all cameras\n        // This is a simplified example - you would implement proper frame combining\n        Mat combinedFrame = new Mat();\n        \n        // Get frames from all cameras and combine them\n        Mat frontFrame = frontCamera.getLatestFrame();\n        Mat rearFrame = rearCamera.getLatestFrame();\n        Mat sideFrame = sideCamera.getLatestFrame();\n        \n        // Combine frames into a 2x2 grid\n        // Implementation would depend on your specific needs\n        \n        return combinedFrame;\n    }\n    \n    @Override\n    public void loop() {\n        // Handle camera switching\n        handleCameraSwitching();\n        \n        // Update camera status\n        updateMultiCameraStatus();\n        \n        // Normal robot control\n        // ...\n        \n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void handleCameraSwitching() {\n        // Switch camera based on configuration\n        if (!ACTIVE_CAMERA.equals(currentActiveCamera)) {\n            currentActiveCamera = ACTIVE_CAMERA;\n            \n            // Update multi-view setting\n            multiViewEnabled = \"ALL\".equals(ACTIVE_CAMERA);\n            \n            packet.put(\"Action\", \"Camera switched to \" + currentActiveCamera);\n        }\n    }\n    \n    private void updateMultiCameraStatus() {\n        packet.put(\"Active Camera\", currentActiveCamera);\n        packet.put(\"Multi-view Enabled\", multiViewEnabled);\n        \n        // Status for each camera\n        for (Map.Entry<String, VisionPortal> entry : cameras.entrySet()) {\n            String cameraName = entry.getKey();\n            VisionPortal camera = entry.getValue();\n            \n            packet.put(cameraName + \" Status\", camera.getCameraState().toString());\n            packet.put(cameraName + \" FPS\", camera.getFps());\n            packet.put(cameraName + \" Frame Count\", camera.getFrameCount());\n        }\n    }\n    \n    @Override\n    public void stop() {\n        // Clean up all cameras\n        for (VisionPortal camera : cameras.values()) {\n            if (camera != null) {\n                camera.close();\n            }\n        }\n    }\n    \n    // Vision processors for different cameras\n    private static class NavigationProcessor implements VisionProcessor {\n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize navigation processing\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame for navigation (AprilTags, field lines, etc.)\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw navigation information on frame\n        }\n    }\n    \n    private static class GameElementProcessor implements VisionProcessor {\n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize game element detection\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame for game elements (cones, cubes, etc.)\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw game element detection results\n        }\n    }\n    \n    private static class ObstacleProcessor implements VisionProcessor {\n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize obstacle detection\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame for obstacle detection\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw obstacle detection results\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Camera Configuration",
      "content": "Camera configuration is crucial for optimal performance. You can adjust resolution, frame rate, exposure, white balance, and other settings to balance quality and performance for your specific use case."
    },
    {
      "type": "code",
      "title": "Camera Configuration Example",
      "content": "This example shows how to configure camera settings for optimal performance:",
      "code": "import com.acmerobotics.dashboard.FtcDashboard;\nimport com.acmerobotics.dashboard.telemetry.TelemetryPacket;\nimport com.acmerobotics.dashboard.config.Config;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.vision.VisionPortal;\nimport org.firstinspires.ftc.vision.VisionProcessor;\nimport org.opencv.core.Mat;\nimport org.opencv.core.Size;\n\n@Config\npublic class ConfigurableCameraOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    \n    // Camera configuration variables\n    public static int CAMERA_WIDTH = 640;\n    public static int CAMERA_HEIGHT = 480;\n    public static int CAMERA_FPS = 30;\n    public static double EXPOSURE_TIME = 10.0; // milliseconds\n    public static double GAIN = 1.0;\n    public static boolean AUTO_EXPOSURE = true;\n    public static boolean AUTO_WHITE_BALANCE = true;\n    public static String CAMERA_FORMAT = \"YUV\"; // YUV, RGB, BGR\n    \n    // Performance settings\n    public static boolean ENABLE_COMPRESSION = true;\n    public static int COMPRESSION_QUALITY = 80; // 0-100\n    public static boolean ENABLE_FRAME_SKIPPING = false;\n    public static int FRAME_SKIP_INTERVAL = 2;\n    \n    private VisionPortal visionPortal;\n    private CameraStreamServer streamServer;\n    private int frameCount = 0;\n    private long lastConfigUpdate = 0;\n    \n    @Override\n    public void init() {\n        dashboard = FtcDashboard.getInstance();\n        packet = new TelemetryPacket();\n        \n        // Initialize camera with configuration\n        initConfigurableCamera();\n        \n        // Set up streaming with performance options\n        setupOptimizedStreaming();\n        \n        packet.put(\"Status\", \"Configurable camera initialized\");\n        packet.put(\"Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n        packet.put(\"FPS\", CAMERA_FPS);\n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void initConfigurableCamera() {\n        // Get camera from hardware map\n        WebcamName webcamName = hardwareMap.get(WebcamName.class, \"webcam\");\n        \n        // Create vision portal with custom processor\n        visionPortal = VisionPortal.easyCreateWithDefaults(webcamName, new ConfigurableVisionProcessor());\n        \n        // Apply initial configuration\n        applyCameraConfiguration();\n    }\n    \n    private void applyCameraConfiguration() {\n        // Set camera resolution\n        visionPortal.setCameraResolution(new Size(CAMERA_WIDTH, CAMERA_HEIGHT));\n        \n        // Set frame rate\n        visionPortal.setCameraFrameRate(CAMERA_FPS);\n        \n        // Configure camera settings if supported\n        try {\n            // Set exposure (if supported by camera)\n            if (!AUTO_EXPOSURE) {\n                visionPortal.setCameraExposure(EXPOSURE_TIME, TimeUnit.MILLISECONDS);\n            } else {\n                visionPortal.setCameraAutoExposure();\n            }\n            \n            // Set gain (if supported)\n            visionPortal.setCameraGain(GAIN);\n            \n            // Set white balance (if supported)\n            if (AUTO_WHITE_BALANCE) {\n                visionPortal.setCameraAutoWhiteBalance();\n            } else {\n                visionPortal.setCameraManualWhiteBalance();\n            }\n            \n        } catch (Exception e) {\n            telemetry.addData(\"Config Warning\", \"Some camera settings not supported: \" + e.getMessage());\n        }\n    }\n    \n    private void setupOptimizedStreaming() {\n        streamServer = CameraStreamServer.getInstance();\n        \n        // Create optimized stream source\n        streamServer.setSource(new CameraStreamSource() {\n            @Override\n            public Mat getFrame() {\n                // Apply frame skipping if enabled\n                if (ENABLE_FRAME_SKIPPING && frameCount % FRAME_SKIP_INTERVAL != 0) {\n                    return null; // Skip this frame\n                }\n                \n                Mat frame = visionPortal.getLatestFrame();\n                frameCount++;\n                \n                // Apply compression if enabled\n                if (ENABLE_COMPRESSION && frame != null) {\n                    return compressFrame(frame);\n                }\n                \n                return frame;\n            }\n            \n            @Override\n            public int getFrameWidth() {\n                return CAMERA_WIDTH;\n            }\n            \n            @Override\n            public int getFrameHeight() {\n                return CAMERA_HEIGHT;\n            }\n            \n            @Override\n            public int getFps() {\n                return ENABLE_FRAME_SKIPPING ? CAMERA_FPS / FRAME_SKIP_INTERVAL : CAMERA_FPS;\n            }\n        });\n        \n        streamServer.start();\n    }\n    \n    private Mat compressFrame(Mat frame) {\n        // Apply compression to reduce bandwidth\n        // This is a simplified example - you would implement actual compression\n        Mat compressedFrame = new Mat();\n        \n        // Convert to compressed format (JPEG-like compression)\n        // Implementation would depend on your compression needs\n        \n        return compressedFrame;\n    }\n    \n    @Override\n    public void loop() {\n        // Check for configuration changes\n        if (System.currentTimeMillis() - lastConfigUpdate > 1000) {\n            applyCameraConfiguration();\n            lastConfigUpdate = System.currentTimeMillis();\n        }\n        \n        // Update camera status\n        updateCameraStatus();\n        \n        // Normal robot control\n        // ...\n        \n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void updateCameraStatus() {\n        packet.put(\"Camera Status\", visionPortal.getCameraState().toString());\n        packet.put(\"Frame Count\", frameCount);\n        packet.put(\"Actual FPS\", visionPortal.getFps());\n        packet.put(\"Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n        packet.put(\"Exposure\", AUTO_EXPOSURE ? \"Auto\" : EXPOSURE_TIME + \"ms\");\n        packet.put(\"Gain\", GAIN);\n        packet.put(\"Compression\", ENABLE_COMPRESSION ? \"On (\" + COMPRESSION_QUALITY + \"%)\" : \"Off\");\n        packet.put(\"Frame Skipping\", ENABLE_FRAME_SKIPPING ? \"On (\" + FRAME_SKIP_INTERVAL + \")\" : \"Off\");\n        \n        // Performance metrics\n        packet.put(\"Memory Usage\", getMemoryUsage() + \" MB\");\n        packet.put(\"Network Bandwidth\", getBandwidthUsage() + \" Mbps\");\n    }\n    \n    private double getMemoryUsage() {\n        // Calculate memory usage (simplified)\n        Runtime runtime = Runtime.getRuntime();\n        long usedMemory = runtime.totalMemory() - runtime.freeMemory();\n        return usedMemory / (1024.0 * 1024.0);\n    }\n    \n    private double getBandwidthUsage() {\n        // Calculate bandwidth usage (simplified)\n        // In a real implementation, you would track actual network usage\n        return (CAMERA_WIDTH * CAMERA_HEIGHT * 3 * CAMERA_FPS) / (1024.0 * 1024.0 * 8.0);\n    }\n    \n    @Override\n    public void stop() {\n        if (visionPortal != null) {\n            visionPortal.close();\n        }\n        if (streamServer != null) {\n            streamServer.stop();\n        }\n    }\n    \n    // Configurable vision processor\n    private static class ConfigurableVisionProcessor implements VisionProcessor {\n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize with configuration\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame based on configuration\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw based on configuration\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Vision Processing Integration",
      "content": "One of the most powerful features is the ability to overlay vision processing results on the camera stream. This allows you to see detected objects, AprilTags, field lines, and other vision processing results in real-time."
    },
    {
      "type": "code",
      "title": "Vision Processing Integration Example",
      "content": "This example demonstrates how to integrate vision processing results with camera streaming:",
      "code": "import com.acmerobotics.dashboard.FtcDashboard;\nimport com.acmerobotics.dashboard.telemetry.TelemetryPacket;\nimport com.acmerobotics.dashboard.config.Config;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.vision.VisionPortal;\nimport org.firstinspires.ftc.vision.VisionProcessor;\nimport org.firstinspires.ftc.vision.apriltag.AprilTagDetection;\nimport org.firstinspires.ftc.vision.apriltag.AprilTagProcessor;\nimport org.opencv.core.Mat;\nimport org.opencv.core.Point;\nimport org.opencv.core.Scalar;\nimport org.opencv.imgproc.Imgproc;\nimport java.util.List;\n\n@Config\npublic class VisionIntegrationOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    \n    // Vision processing configuration\n    public static boolean ENABLE_APRILTAG_DETECTION = true;\n    public static boolean ENABLE_COLOR_DETECTION = true;\n    public static boolean ENABLE_LINE_DETECTION = true;\n    public static boolean SHOW_DETECTION_OVERLAY = true;\n    \n    // Detection parameters\n    public static double COLOR_THRESHOLD = 0.5;\n    public static int LINE_THRESHOLD = 100;\n    public static double APRILTAG_CONFIDENCE = 0.8;\n    \n    private VisionPortal visionPortal;\n    private AprilTagProcessor aprilTagProcessor;\n    private ColorDetectionProcessor colorProcessor;\n    private LineDetectionProcessor lineProcessor;\n    private CameraStreamServer streamServer;\n    \n    // Detection results for overlay\n    private List<AprilTagDetection> aprilTagDetections;\n    private List<ColorDetection> colorDetections;\n    private List<LineDetection> lineDetections;\n    \n    @Override\n    public void init() {\n        dashboard = FtcDashboard.getInstance();\n        packet = new TelemetryPacket();\n        \n        // Initialize vision processors\n        initVisionProcessors();\n        \n        // Set up camera with vision processing\n        setupVisionCamera();\n        \n        // Set up streaming with overlay\n        setupVisionStreaming();\n        \n        packet.put(\"Status\", \"Vision processing initialized\");\n        packet.put(\"AprilTag Detection\", ENABLE_APRILTAG_DETECTION);\n        packet.put(\"Color Detection\", ENABLE_COLOR_DETECTION);\n        packet.put(\"Line Detection\", ENABLE_LINE_DETECTION);\n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void initVisionProcessors() {\n        // Initialize AprilTag processor\n        if (ENABLE_APRILTAG_DETECTION) {\n            aprilTagProcessor = AprilTagProcessor.easyCreateWithDefaults();\n            aprilTagProcessor.setDecimation(2.0);\n            aprilTagProcessor.setMaxHammingDistance(1);\n        }\n        \n        // Initialize color detection processor\n        if (ENABLE_COLOR_DETECTION) {\n            colorProcessor = new ColorDetectionProcessor();\n        }\n        \n        // Initialize line detection processor\n        if (ENABLE_LINE_DETECTION) {\n            lineProcessor = new LineDetectionProcessor();\n        }\n    }\n    \n    private void setupVisionCamera() {\n        WebcamName webcamName = hardwareMap.get(WebcamName.class, \"webcam\");\n        \n        // Create vision portal with multiple processors\n        VisionPortal.Builder builder = new VisionPortal.Builder();\n        builder.setCamera(webcamName);\n        \n        if (ENABLE_APRILTAG_DETECTION) {\n            builder.addProcessor(aprilTagProcessor);\n        }\n        if (ENABLE_COLOR_DETECTION) {\n            builder.addProcessor(colorProcessor);\n        }\n        if (ENABLE_LINE_DETECTION) {\n            builder.addProcessor(lineProcessor);\n        }\n        \n        visionPortal = builder.build();\n    }\n    \n    private void setupVisionStreaming() {\n        streamServer = CameraStreamServer.getInstance();\n        \n        streamServer.setSource(new CameraStreamSource() {\n            @Override\n            public Mat getFrame() {\n                Mat frame = visionPortal.getLatestFrame();\n                \n                if (frame != null && SHOW_DETECTION_OVERLAY) {\n                    // Add vision processing overlays to the frame\n                    addVisionOverlays(frame);\n                }\n                \n                return frame;\n            }\n            \n            @Override\n            public int getFrameWidth() {\n                return 640;\n            }\n            \n            @Override\n            public int getFrameHeight() {\n                return 480;\n            }\n            \n            @Override\n            public int getFps() {\n                return 30;\n            }\n        });\n        \n        streamServer.start();\n    }\n    \n    private void addVisionOverlays(Mat frame) {\n        // Add AprilTag detection overlays\n        if (ENABLE_APRILTAG_DETECTION && aprilTagDetections != null) {\n            for (AprilTagDetection detection : aprilTagDetections) {\n                if (detection.confidence >= APRILTAG_CONFIDENCE) {\n                    // Draw AprilTag bounding box\n                    Point[] corners = detection.corners;\n                    for (int i = 0; i < 4; i++) {\n                        Point start = corners[i];\n                        Point end = corners[(i + 1) % 4];\n                        Imgproc.line(frame, start, end, new Scalar(0, 255, 0), 2);\n                    }\n                    \n                    // Draw AprilTag ID and confidence\n                    Point center = detection.center;\n                    String label = \"Tag \" + detection.id + \" (\" + \n                                  String.format(\"%.2f\", detection.confidence) + \")\";\n                    Imgproc.putText(frame, label, center, \n                                   Imgproc.FONT_HERSHEY_SIMPLEX, 0.5, \n                                   new Scalar(0, 255, 0), 1);\n                }\n            }\n        }\n        \n        // Add color detection overlays\n        if (ENABLE_COLOR_DETECTION && colorDetections != null) {\n            for (ColorDetection detection : colorDetections) {\n                // Draw color detection circles\n                Imgproc.circle(frame, detection.center, detection.radius, \n                              detection.color, 2);\n                \n                // Draw color label\n                String label = detection.colorName + \" (\" + \n                              String.format(\"%.2f\", detection.confidence) + \")\";\n                Imgproc.putText(frame, label, \n                               new Point(detection.center.x, detection.center.y - 10), \n                               Imgproc.FONT_HERSHEY_SIMPLEX, 0.4, \n                               detection.color, 1);\n            }\n        }\n        \n        // Add line detection overlays\n        if (ENABLE_LINE_DETECTION && lineDetections != null) {\n            for (LineDetection detection : lineDetections) {\n                // Draw detected lines\n                Imgproc.line(frame, detection.start, detection.end, \n                            new Scalar(255, 0, 0), 2);\n                \n                // Draw line angle\n                Point midPoint = new Point((detection.start.x + detection.end.x) / 2, \n                                         (detection.start.y + detection.end.y) / 2);\n                String angleLabel = String.format(\"%.1fÂ°\", detection.angle);\n                Imgproc.putText(frame, angleLabel, midPoint, \n                               Imgproc.FONT_HERSHEY_SIMPLEX, 0.4, \n                               new Scalar(255, 0, 0), 1);\n            }\n        }\n    }\n    \n    @Override\n    public void loop() {\n        // Update detection results\n        updateDetectionResults();\n        \n        // Update vision status\n        updateVisionStatus();\n        \n        // Normal robot control\n        // ...\n        \n        dashboard.sendTelemetryPacket(packet);\n    }\n    \n    private void updateDetectionResults() {\n        // Get AprilTag detections\n        if (ENABLE_APRILTAG_DETECTION && aprilTagProcessor != null) {\n            aprilTagDetections = aprilTagProcessor.getDetections();\n        }\n        \n        // Get color detections (from custom processor)\n        if (ENABLE_COLOR_DETECTION && colorProcessor != null) {\n            colorDetections = colorProcessor.getDetections();\n        }\n        \n        // Get line detections (from custom processor)\n        if (ENABLE_LINE_DETECTION && lineProcessor != null) {\n            lineDetections = lineProcessor.getDetections();\n        }\n    }\n    \n    private void updateVisionStatus() {\n        packet.put(\"Vision Status\", visionPortal.getCameraState().toString());\n        packet.put(\"AprilTag Count\", aprilTagDetections != null ? aprilTagDetections.size() : 0);\n        packet.put(\"Color Detection Count\", colorDetections != null ? colorDetections.size() : 0);\n        packet.put(\"Line Detection Count\", lineDetections != null ? lineDetections.size() : 0);\n        packet.put(\"Overlay Enabled\", SHOW_DETECTION_OVERLAY);\n        \n        // Show detection details\n        if (aprilTagDetections != null && !aprilTagDetections.isEmpty()) {\n            AprilTagDetection bestDetection = aprilTagDetections.get(0);\n            packet.put(\"Best AprilTag\", \"ID: \" + bestDetection.id + \n                       \", Confidence: \" + String.format(\"%.2f\", bestDetection.confidence));\n        }\n    }\n    \n    @Override\n    public void stop() {\n        if (visionPortal != null) {\n            visionPortal.close();\n        }\n        if (streamServer != null) {\n            streamServer.stop();\n        }\n    }\n    \n    // Custom detection classes (simplified examples)\n    private static class ColorDetection {\n        Point center;\n        int radius;\n        Scalar color;\n        String colorName;\n        double confidence;\n    }\n    \n    private static class LineDetection {\n        Point start;\n        Point end;\n        double angle;\n        double confidence;\n    }\n    \n    // Custom vision processors (simplified implementations)\n    private static class ColorDetectionProcessor implements VisionProcessor {\n        private List<ColorDetection> detections = new ArrayList<>();\n        \n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize color detection\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame for color detection\n            // Implementation would include actual color detection logic\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw color detection results\n        }\n        \n        public List<ColorDetection> getDetections() {\n            return detections;\n        }\n    }\n    \n    private static class LineDetectionProcessor implements VisionProcessor {\n        private List<LineDetection> detections = new ArrayList<>();\n        \n        @Override\n        public void init(int width, int height, CameraCalibration calibration) {\n            // Initialize line detection\n        }\n        \n        @Override\n        public Object processFrame(Mat frame, long captureTimeNanos) {\n            // Process frame for line detection\n            // Implementation would include actual line detection logic\n            return null;\n        }\n        \n        @Override\n        public void onDrawFrame(Canvas canvas, int onscreenWidth, int onscreenHeight, \n                               float scaleBmpPxToCanvasPx, float scaleCanvasDensity, \n                               Object userContext) {\n            // Draw line detection results\n        }\n        \n        public List<LineDetection> getDetections() {\n            return detections;\n        }\n    }\n}"
    },
    {
      "type": "rules-box",
      "title": "Camera Streaming Best Practices",
      "items": [
        "Choose appropriate resolution and frame rate for your use case",
        "Use compression to reduce bandwidth when needed",
        "Implement frame skipping for performance-critical applications",
        "Configure camera settings for optimal image quality",
        "Handle multiple cameras efficiently with proper resource management",
        "Integrate vision processing results for enhanced debugging",
        "Monitor performance and adjust settings as needed"
      ]
    },
    {
      "type": "exercise-box",
      "title": "Practice Exercise: Multi-Camera Vision System",
      "description": "Create an OpMode with multiple cameras, vision processing, and real-time streaming to the dashboard.",
      "tasks": [
        "Set up multiple cameras with different purposes",
        "Implement vision processing for each camera",
        "Create camera switching and multi-view functionality",
        "Add vision processing overlays to the stream",
        "Configure camera settings for optimal performance"
      ],
      "content": "// Your OpMode should include:\n// 1. Multiple camera initialization and management\n// 2. Vision processing for different purposes\n// 3. Camera switching and multi-view support\n// 4. Real-time overlay of detection results\n// 5. Performance optimization and configuration"
    },
    {
      "type": "rules-box",
      "title": "Troubleshooting Camera Streaming",
      "items": [
        "No video stream: Check camera hardware connections and initialization",
        "Poor performance: Reduce resolution, frame rate, or enable compression",
        "High bandwidth usage: Enable compression and frame skipping",
        "Vision processing not working: Verify processor initialization and configuration",
        "Multiple camera issues: Check hardware compatibility and resource allocation",
        "Overlay not appearing: Ensure vision processors are properly configured"
      ]
    },
    {
      "type": "link-grid",
      "title": "Related Topics and Resources",
      "links": [
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera\" target=\"_blank\">FTCDashboard Camera Streaming Documentation</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#multiple-cameras\" target=\"_blank\">Multiple Camera Support</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#configuration\" target=\"_blank\">Camera Configuration Guide</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#vision-integration\" target=\"_blank\">Vision Processing Integration</a>",
        "<a href=\"../vision/vision-introduction.json\">Vision Introduction</a>",
        "<a href=\"../vision/apriltags.json\">AprilTag Detection</a>",
        "<a href=\"../autonomous-programming/basic-autonomous-programming.json\">Basic Autonomous Programming</a>"
      ]
    }
  ]
} 