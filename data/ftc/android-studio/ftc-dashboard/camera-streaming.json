{
  "title": "Camera Streaming",
  "sections": [
    {
      "type": "text",
      "title": "Camera Integration Overview",
      "content": "FTCDashboard provides powerful camera streaming capabilities that allow you to view live video feeds from your robot's cameras directly in the web interface. This is essential for vision-based autonomous programming, debugging, and real-time robot monitoring."
    },
    {
      "type": "rules-box",
      "title": "Camera Streaming Benefits",
      "items": [
        "Live video feed from robot cameras",
        "Real-time vision processing and analysis",
        "Remote camera control and configuration",
        "Multiple camera support",
        "Image capture and storage capabilities",
        "Vision-based autonomous programming support"
      ]
    },
    {
      "type": "text",
      "title": "Step 1: Basic Camera Setup",
      "content": "To use camera streaming with FTCDashboard, you need to set up the camera hardware and configure the streaming parameters. The dashboard automatically handles the video transmission and display."
    },
    {
      "type": "code",
      "title": "Required Imports and Class Setup",
      "content": "Start by adding these imports and setting up your OpMode:",
      "code": "// Essential imports for camera streaming\nimport com.acmerobotics.dashboard.FtcDashboard;\nimport com.acmerobotics.dashboard.telemetry.TelemetryPacket;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.Webcam;\n\npublic class BasicCameraOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    private Webcam webcam;\n    \n    // Camera configuration constants\n    private static final int CAMERA_WIDTH = 640;\n    private static final int CAMERA_HEIGHT = 480;\n    private static final int FPS = 30;\n    \n    // Camera status tracking\n    private boolean cameraActive = false;\n    private int frameCount = 0;"
    },
    {
      "type": "text",
      "title": "Step 2: Camera Initialization",
      "content": "The init() method is where we set up the camera hardware and configure its parameters. This includes setting resolution, frame rate, and other camera settings."
    },
    {
      "type": "code",
      "title": "Camera Initialization Method",
      "content": "Add this to your init() method:",
      "code": "@Override\npublic void init() {\n    dashboard = FtcDashboard.getInstance();\n    packet = new TelemetryPacket();\n    \n    // Initialize camera\n    initializeCamera();\n    \n    // Send initial camera status to dashboard\n    packet.put(\"Camera Status\", cameraActive ? \"Active\" : \"Inactive\");\n    packet.put(\"Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n    packet.put(\"FPS\", FPS);\n    dashboard.sendTelemetryPacket(packet);\n    \n    // Update telemetry\n    telemetry.addData(\"Status\", cameraActive ? \"Camera initialized\" : \"Camera failed\");\n    telemetry.addData(\"Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n    telemetry.addData(\"FPS\", FPS);\n    telemetry.update();\n}\n\nprivate void initializeCamera() {\n    try {\n        // Get webcam from hardware map\n        webcam = hardwareMap.get(Webcam.class, \"webcam\");\n        \n        // Configure camera settings\n        webcam.setViewportRenderer(Webcam.ViewportRenderer.BUFFERED);\n        webcam.setMillisecondsPermissionTimeout(2500);\n        webcam.setResolution(CAMERA_WIDTH, CAMERA_HEIGHT);\n        webcam.setFps(FPS);\n        \n        // Check if camera is active\n        cameraActive = webcam.isOpen();\n        \n        telemetry.addData(\"Camera\", cameraActive ? \"Webcam initialized\" : \"Camera not found\");\n    } catch (Exception e) {\n        telemetry.addData(\"Camera Error\", \"Failed to initialize: \" + e.getMessage());\n        cameraActive = false;\n    }\n}"
    },
    {
      "type": "text",
      "title": "Step 3: Camera Status Monitoring",
      "content": "In the loop() method, we'll monitor the camera status and send updates to the dashboard. This allows you to see if the camera is working properly and track its performance."
    },
    {
      "type": "code",
      "title": "Camera Status Monitoring",
      "content": "Add this to your loop() method:",
      "code": "@Override\npublic void loop() {\n    // Update camera status\n    updateCameraStatus();\n    \n    // Update frame count\n    frameCount++;\n    \n    // Send camera data to dashboard\n    sendCameraData();\n    \n    // Update telemetry\n    telemetry.addData(\"Camera Active\", cameraActive);\n    telemetry.addData(\"Frame Count\", frameCount);\n    telemetry.addData(\"Runtime\", \"%.2f\", getRuntime());\n    telemetry.update();\n}\n\nprivate void updateCameraStatus() {\n    // Check if camera is still active\n    if (webcam != null) {\n        cameraActive = webcam.isOpen();\n    }\n}\n\nprivate void sendCameraData() {\n    packet.put(\"Camera Active\", cameraActive);\n    packet.put(\"Frame Count\", frameCount);\n    packet.put(\"Runtime\", getRuntime());\n    packet.put(\"Camera Resolution\", CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT);\n    \n    dashboard.sendTelemetryPacket(packet);\n}"
    },
    {
      "type": "text",
      "title": "Step 4: Camera Cleanup",
      "content": "It's important to properly close the camera when the OpMode stops to free up resources and prevent issues with future camera usage."
    },
    {
      "type": "code",
      "title": "Camera Cleanup Method",
      "content": "Add this to your stop() method:",
      "code": "@Override\npublic void stop() {\n    // Close camera properly\n    if (webcam != null && webcam.isOpen()) {\n        webcam.close();\n        cameraActive = false;\n    }\n    \n    // Send final status to dashboard\n    packet.put(\"Camera Status\", \"Closed\");\n    packet.put(\"Final Frame Count\", frameCount);\n    dashboard.sendTelemetryPacket(packet);\n    \n    // Update telemetry\n    telemetry.addData(\"Status\", \"Camera closed\");\n    telemetry.addData(\"Total Frames\", frameCount);\n    telemetry.update();\n}"
    },
    {
      "type": "text",
      "title": "Step 5: Advanced Camera Configuration",
      "content": "For more advanced applications, you can configure multiple cameras, adjust quality settings, and add configuration variables for real-time adjustment."
    },
    {
      "type": "code",
      "title": "Advanced Camera Configuration Setup",
      "content": "Add these configuration variables and methods:",
      "code": "import com.acmerobotics.dashboard.config.Config;\nimport com.acmerobotics.dashboard.config.Value;\n\n@Config\npublic class AdvancedCameraOpMode extends OpMode {\n    private FtcDashboard dashboard;\n    private TelemetryPacket packet;\n    private Webcam primaryCamera;\n    private Webcam secondaryCamera;\n    \n    // Configurable camera settings\n    public static int CAMERA_WIDTH = 640;\n    public static int CAMERA_HEIGHT = 480;\n    public static int CAMERA_FPS = 30;\n    public static int CAMERA_QUALITY = 80;\n    public static boolean USE_SECONDARY_CAMERA = false;\n    public static String PRIMARY_CAMERA_NAME = \"webcam\";\n    public static String SECONDARY_CAMERA_NAME = \"webcam2\";\n    \n    // Camera status tracking\n    private boolean camerasInitialized = false;\n    private int frameCount = 0;\n    private long lastFrameTime = 0;"
    },
    {
      "type": "text",
      "title": "Step 6: Multiple Camera Initialization",
      "content": "Now we'll set up multiple cameras with configurable parameters. This allows you to use different cameras for different purposes."
    },
    {
      "type": "code",
      "title": "Multiple Camera Setup",
      "content": "Add this method to initialize multiple cameras:",
      "code": "private void initializeCameras() {\n    try {\n        // Initialize primary camera\n        primaryCamera = hardwareMap.get(Webcam.class, PRIMARY_CAMERA_NAME);\n        configureCamera(primaryCamera, \"Primary\");\n        \n        // Initialize secondary camera if enabled\n        if (USE_SECONDARY_CAMERA) {\n            try {\n                secondaryCamera = hardwareMap.get(Webcam.class, SECONDARY_CAMERA_NAME);\n                configureCamera(secondaryCamera, \"Secondary\");\n            } catch (Exception e) {\n                telemetry.addData(\"Secondary Camera\", \"Not available: \" + e.getMessage());\n            }\n        }\n        \n        camerasInitialized = true;\n        telemetry.addData(\"Status\", \"Cameras initialized successfully\");\n    } catch (Exception e) {\n        telemetry.addData(\"Error\", \"Failed to initialize cameras: \" + e.getMessage());\n        camerasInitialized = false;\n    }\n}\n\nprivate void configureCamera(Webcam camera, String cameraName) {\n    // Set basic camera configuration\n    camera.setViewportRenderer(Webcam.ViewportRenderer.BUFFERED);\n    camera.setMillisecondsPermissionTimeout(5000);\n    camera.setResolution(CAMERA_WIDTH, CAMERA_HEIGHT);\n    camera.setFps(CAMERA_FPS);\n    \n    // Try to set quality (may not be supported on all cameras)\n    try {\n        camera.setImageQuality(CAMERA_QUALITY);\n    } catch (Exception e) {\n        // Quality setting not supported\n        telemetry.addData(cameraName + \" Camera\", \"Quality setting not supported\");\n    }\n    \n    telemetry.addData(cameraName + \" Camera\", \"Configured: \" + CAMERA_WIDTH + \"x\" + CAMERA_HEIGHT + \" @ \" + CAMERA_FPS + \"fps\");\n}"
    },
    {
      "type": "text",
      "title": "Step 7: Camera Performance Monitoring",
      "content": "We'll add methods to monitor camera performance, including frame rate calculation and status checking."
    },
    {
      "type": "code",
      "title": "Camera Performance Monitoring",
      "content": "Add these methods to monitor camera performance:",
      "code": "private void updateFrameStatistics() {\n    frameCount++;\n    long currentTime = System.currentTimeMillis();\n    if (currentTime - lastFrameTime >= 1000) {\n        lastFrameTime = currentTime;\n    }\n}\n\nprivate void checkCameraStatus() {\n    // Check primary camera status\n    if (primaryCamera != null && !primaryCamera.isOpen()) {\n        telemetry.addData(\"Warning\", \"Primary camera disconnected\");\n    }\n    \n    // Check secondary camera status\n    if (USE_SECONDARY_CAMERA && secondaryCamera != null && !secondaryCamera.isOpen()) {\n        telemetry.addData(\"Warning\", \"Secondary camera disconnected\");\n    }\n}\n\nprivate boolean isCameraActive(Webcam camera) {\n    return camera != null && camera.isOpen();\n}\n\nprivate double calculateFPS() {\n    long currentTime = System.currentTimeMillis();\n    if (lastFrameTime > 0) {\n        return 1000.0 / (currentTime - lastFrameTime);\n    }\n    return 0.0;\n}\n\nprivate void sendCameraConfiguration() {\n    packet.put(\"Camera Width\", CAMERA_WIDTH);\n    packet.put(\"Camera Height\", CAMERA_HEIGHT);\n    packet.put(\"Camera FPS\", CAMERA_FPS);\n    packet.put(\"Camera Quality\", CAMERA_QUALITY);\n    packet.put(\"Use Secondary Camera\", USE_SECONDARY_CAMERA);\n    packet.put(\"Cameras Initialized\", camerasInitialized);\n    dashboard.sendTelemetryPacket(packet);\n}\n\nprivate void sendCameraData() {\n    packet.put(\"Frame Count\", frameCount);\n    packet.put(\"Calculated FPS\", calculateFPS());\n    packet.put(\"Primary Camera Active\", isCameraActive(primaryCamera));\n    \n    if (USE_SECONDARY_CAMERA && secondaryCamera != null) {\n        packet.put(\"Secondary Camera Active\", isCameraActive(secondaryCamera));\n    }\n    \n    dashboard.sendTelemetryPacket(packet);\n}"
    },
    {
      "type": "text",
      "title": "Step 8: Image Processing Integration",
      "content": "FTCDashboard can integrate with image processing libraries to provide real-time analysis of camera feeds. This is essential for vision-based autonomous programming and object detection."
    },
    {
      "type": "code",
      "title": "Basic Image Processing Setup",
      "content": "Add these variables and methods for image processing:",
      "code": "// Image processing configuration\npublic static double COLOR_THRESHOLD_RED = 100;\npublic static double COLOR_THRESHOLD_GREEN = 100;\npublic static double COLOR_THRESHOLD_BLUE = 100;\npublic static boolean ENABLE_EDGE_DETECTION = false;\npublic static boolean ENABLE_COLOR_DETECTION = true;\npublic static int BLUR_KERNEL_SIZE = 5;\n\n// Processing status\nprivate boolean processingEnabled = true;\nprivate int processedFrameCount = 0;\n\n// Processing results\nprivate double redIntensity = 0;\nprivate double greenIntensity = 0;\nprivate double blueIntensity = 0;\nprivate boolean redDetected = false;\nprivate boolean greenDetected = false;\nprivate boolean blueDetected = false;"
    },
    {
      "type": "text",
      "title": "Step 9: Image Processing Methods",
      "content": "Now we'll add methods to process camera frames and detect objects or colors. This is a simplified example that simulates image processing."
    },
    {
      "type": "code",
      "title": "Image Processing Implementation",
      "content": "Add these methods for image processing:",
      "code": "private void processFrame() {\n    if (!processingEnabled || !cameraActive) {\n        return;\n    }\n    \n    processedFrameCount++;\n    \n    // Simulate color detection (in practice, you'd analyze actual camera frames)\n    if (ENABLE_COLOR_DETECTION) {\n        // Simulate color intensity values\n        redIntensity = Math.random() * 255;\n        greenIntensity = Math.random() * 255;\n        blueIntensity = Math.random() * 255;\n        \n        // Check if colors exceed thresholds\n        redDetected = redIntensity > COLOR_THRESHOLD_RED;\n        greenDetected = greenIntensity > COLOR_THRESHOLD_GREEN;\n        blueDetected = blueIntensity > COLOR_THRESHOLD_BLUE;\n        \n        // Send color data to dashboard\n        packet.put(\"Red Intensity\", redIntensity);\n        packet.put(\"Green Intensity\", greenIntensity);\n        packet.put(\"Blue Intensity\", blueIntensity);\n        packet.put(\"Red Detected\", redDetected);\n        packet.put(\"Green Detected\", greenDetected);\n        packet.put(\"Blue Detected\", blueDetected);\n    }\n    \n    // Simulate edge detection\n    if (ENABLE_EDGE_DETECTION) {\n        int edgeCount = (int) (Math.random() * 100);\n        packet.put(\"Edge Count\", edgeCount);\n    }\n    \n    // Simulate object detection\n    simulateObjectDetection();\n}\n\nprivate void simulateObjectDetection() {\n    int objectCount = (int) (Math.random() * 5);\n    packet.put(\"Object Count\", objectCount);\n    \n    for (int i = 0; i < objectCount; i++) {\n        double x = Math.random() * CAMERA_WIDTH;\n        double y = Math.random() * CAMERA_HEIGHT;\n        double size = Math.random() * 50 + 10;\n        \n        packet.put(\"Object \" + i + \" X\", x);\n        packet.put(\"Object \" + i + \" Y\", y);\n        packet.put(\"Object \" + i + \" Size\", size);\n    }\n}\n\nprivate void updateProcessingStats() {\n    packet.put(\"Processed Frame Count\", processedFrameCount);\n    packet.put(\"Processing Time\", getRuntime());\n    packet.put(\"Processing Enabled\", processingEnabled);\n}\n\nprivate void sendProcessingConfiguration() {\n    packet.put(\"Color Threshold Red\", COLOR_THRESHOLD_RED);\n    packet.put(\"Color Threshold Green\", COLOR_THRESHOLD_GREEN);\n    packet.put(\"Color Threshold Blue\", COLOR_THRESHOLD_BLUE);\n    packet.put(\"Enable Edge Detection\", ENABLE_EDGE_DETECTION);\n    packet.put(\"Enable Color Detection\", ENABLE_COLOR_DETECTION);\n    packet.put(\"Blur Kernel Size\", BLUR_KERNEL_SIZE);\n    dashboard.sendTelemetryPacket(packet);\n}"
    },
    {
      "type": "text",
      "title": "Step 10: Vision-Based Control",
      "content": "Camera streaming can be integrated with robot control to create vision-based autonomous behaviors. This allows the robot to make decisions based on what it sees through the camera."
    },
    {
      "type": "code",
      "title": "Vision-Based Control Setup",
      "content": "Add these variables and methods for vision-based control:",
      "code": "// Vision control configuration\npublic static double TARGET_COLOR_RED = 150;\npublic static double TARGET_COLOR_GREEN = 100;\npublic static double TARGET_COLOR_BLUE = 50;\npublic static double COLOR_TOLERANCE = 30;\npublic static double VISION_CONTROL_SPEED = 0.3;\npublic static boolean ENABLE_VISION_CONTROL = true;\n\n// Vision control variables\nprivate boolean targetDetected = false;\nprivate double targetX = 0;\nprivate double targetY = 0;\nprivate double targetSize = 0;\nprivate String detectedColor = \"None\";\n\n// Control mode enum\nprivate enum ControlMode {\n    MANUAL,\n    VISION_FOLLOW,\n    VISION_AVOID,\n    COLOR_SEEK\n}\nprivate ControlMode currentMode = ControlMode.MANUAL;\n\n// Hardware objects for vision control\nprivate DcMotor leftMotor, rightMotor;"
    },
    {
      "type": "exercise-box",
      "title": "Practice Exercise: Vision-Based Autonomous",
      "description": "Create an OpMode that uses camera streaming for vision-based autonomous navigation.",
      "tasks": [
        "Set up camera streaming with FTCDashboard",
        "Implement basic image processing (color detection)",
        "Create vision-based control algorithms",
        "Integrate vision control with robot movement",
        "Test and tune vision parameters using the dashboard"
      ],
      "content": "// Your OpMode should include:\n// 1. Camera initialization and streaming setup\n// 2. Image processing and analysis\n// 3. Vision-based control algorithms\n// 4. Integration with robot movement\n// 5. Real-time parameter tuning via dashboard"
    },
    {
      "type": "rules-box",
      "title": "Camera Streaming Best Practices",
      "items": [
        "Use appropriate resolution and frame rate for your application",
        "Implement proper error handling for camera initialization",
        "Monitor camera performance and adjust settings as needed",
        "Use multiple cameras when additional viewpoints are required",
        "Implement image processing efficiently to avoid performance issues",
        "Provide clear feedback about camera status and processing results"
      ]
    },
    {
      "type": "rules-box",
      "title": "Vision Processing Guidelines",
      "items": [
        "Start with simple color detection before implementing complex algorithms",
        "Use appropriate thresholds and tolerances for reliable detection",
        "Test vision algorithms in various lighting conditions",
        "Implement fallback behaviors when vision processing fails",
        "Optimize processing speed for real-time applications",
        "Document vision parameters and their effects on performance"
      ]
    },
    {
      "type": "rules-box",
      "title": "Troubleshooting Camera Issues",
      "items": [
        "Camera not initializing: Check hardware connections and permissions",
        "Poor video quality: Adjust resolution, frame rate, and quality settings",
        "High latency: Reduce frame rate or processing complexity",
        "Vision processing errors: Verify algorithm parameters and input validation",
        "Performance issues: Optimize image processing algorithms",
        "Connection problems: Check network bandwidth and camera compatibility"
      ]
    },
    {
      "type": "link-grid",
      "title": "Related Topics and Resources",
      "links": [
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera\" target=\"_blank\">FTCDashboard Camera Documentation</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#streaming\" target=\"_blank\">Camera Streaming Guide</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#processing\" target=\"_blank\">Image Processing</a>",
        "<a href=\"https://acmerobotics.github.io/ftc-dashboard/camera#vision-control\" target=\"_blank\">Vision-Based Control</a>",
        "<a href=\"../vision/vision-introduction.json\">Vision Introduction</a>",
        "<a href=\"../vision/opencv-basics.json\">OpenCV Basics</a>",
        "<a href=\"../autonomous-programming/basic-autonomous-programming.json\">Basic Autonomous Programming</a>"
      ]
    }
  ]
} 