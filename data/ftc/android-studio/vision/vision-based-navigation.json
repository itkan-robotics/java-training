{
  "title": "Vision-Based Navigation",
  "sections": [
    {
      "type": "text",
      "title": "Vision in Robot Navigation",
      "content": "Vision-based navigation combines visual information with traditional robot navigation techniques to create more robust and accurate positioning systems. While encoders and IMUs provide good relative positioning, vision can provide absolute positioning references and help with obstacle detection and avoidance.<br><br>Vision-based navigation is particularly valuable in FTC because it can compensate for encoder slip, provide absolute positioning using field landmarks, and enable dynamic obstacle avoidance that traditional navigation systems cannot handle."
    },
    {
      "type": "rules-box",
      "title": "Vision Navigation Advantages",
      "items": [
        "Absolute positioning using visual landmarks",
        "Compensation for encoder slip and wheel wear",
        "Dynamic obstacle detection and avoidance",
        "Enhanced path following accuracy",
        "Real-time field state analysis",
        "Integration with existing navigation systems"
      ]
    },
    {
      "type": "text",
      "title": "Visual Odometry Principles",
      "content": "Visual odometry uses camera images to estimate robot motion by tracking visual features between consecutive frames. It works by detecting distinctive points in images and tracking their movement as the robot moves.<br><br><strong>Visual Odometry Process:</strong><br>1. <strong>Feature Detection:</strong> Find distinctive points in the current image<br>2. <strong>Feature Matching:</strong> Match features between consecutive frames<br>3. <strong>Motion Estimation:</strong> Calculate robot movement from feature displacement<br>4. <strong>Pose Integration:</strong> Integrate motion estimates to track robot position<br><br><strong>Integration with Traditional Odometry:</strong><br>Visual odometry can be combined with wheel encoders and IMU data using sensor fusion techniques like Kalman filtering to provide more accurate and robust positioning."
    },
    {
      "type": "code",
      "title": "Basic Visual Odometry Implementation",
      "content": "Example of a basic visual odometry system:",
      "code": "public class VisualOdometryPipeline extends OpenCvPipeline {\n    \n    // Visual odometry state\n    private Mat previousFrame;\n    private List<Point2f> previousFeatures;\n    private List<Point2f> currentFeatures;\n    private MatOfPoint2f previousPoints;\n    private MatOfPoint2f currentPoints;\n    \n    // Motion estimation results\n    private double estimatedX = 0;\n    private double estimatedY = 0;\n    private double estimatedTheta = 0;\n    \n    // Camera calibration parameters (obtain from calibration)\n    private static final double FOCAL_LENGTH = 578.272;\n    private static final double PIXEL_SIZE = 0.00375; // mm per pixel\n    private static final double CAMERA_HEIGHT = 200; // mm above ground\n    \n    @Override\n    public Mat processFrame(Mat input) {\n        Mat output = input.clone();\n        \n        // Convert to grayscale for feature detection\n        Mat gray = new Mat();\n        Imgproc.cvtColor(input, gray, Imgproc.COLOR_RGB2GRAY);\n        \n        if (previousFrame == null) {\n            // First frame - initialize\n            previousFrame = gray.clone();\n            detectFeatures(gray);\n            return output;\n        }\n        \n        // Detect features in current frame\n        detectFeatures(gray);\n        \n        // Track features between frames\n        if (previousFeatures != null && !previousFeatures.isEmpty()) {\n            trackFeatures(gray);\n            \n            // Estimate motion from feature displacement\n            estimateMotion();\n            \n            // Draw tracking results\n            drawTrackingResults(output);\n        }\n        \n        // Update for next frame\n        previousFrame = gray.clone();\n        if (currentFeatures != null) {\n            previousFeatures = new ArrayList<>(currentFeatures);\n        }\n        \n        gray.release();\n        return output;\n    }\n    \n    private void detectFeatures(Mat gray) {\n        // Use Shi-Tomasi corner detection\n        MatOfPoint2f corners = new MatOfPoint2f();\n        Imgproc.goodFeaturesToTrack(gray, corners, 100, 0.01, 10);\n        \n        currentFeatures = corners.toList();\n        currentPoints = corners;\n    }\n    \n    private void trackFeatures(Mat gray) {\n        if (previousFeatures == null || previousFeatures.isEmpty()) {\n            return;\n        }\n        \n        // Convert previous features to MatOfPoint2f\n        previousPoints = new MatOfPoint2f();\n        previousPoints.fromList(previousFeatures);\n        \n        // Calculate optical flow\n        MatOfPoint2f nextPoints = new MatOfPoint2f();\n        MatOfByte status = new MatOfByte();\n        MatOfFloat err = new MatOfFloat();\n        \n        Imgproc.calcOpticalFlowPyrLK(\n            previousFrame, gray, \n            previousPoints, nextPoints, \n            status, err\n        );\n        \n        // Filter good matches\n        List<Point2f> goodPrevious = new ArrayList<>();\n        List<Point2f> goodCurrent = new ArrayList<>();\n        \n        byte[] statusArray = status.toArray();\n        Point2f[] previousArray = previousPoints.toArray();\n        Point2f[] currentArray = nextPoints.toArray();\n        \n        for (int i = 0; i < statusArray.length; i++) {\n            if (statusArray[i] == 1) {\n                goodPrevious.add(previousArray[i]);\n                goodCurrent.add(currentArray[i]);\n            }\n        }\n        \n        // Update current features with tracked points\n        currentFeatures = goodCurrent;\n        currentPoints = new MatOfPoint2f();\n        currentPoints.fromList(currentFeatures);\n        \n        // Clean up\n        nextPoints.release();\n        status.release();\n        err.release();\n    }\n    \n    private void estimateMotion() {\n        if (previousFeatures == null || currentFeatures == null || \n            previousFeatures.size() < 3 || currentFeatures.size() < 3) {\n            return;\n        }\n        \n        // Calculate average displacement\n        double avgDeltaX = 0;\n        double avgDeltaY = 0;\n        int validMatches = 0;\n        \n        for (int i = 0; i < Math.min(previousFeatures.size(), currentFeatures.size()); i++) {\n            Point2f prev = previousFeatures.get(i);\n            Point2f curr = currentFeatures.get(i);\n            \n            double deltaX = curr.x - prev.x;\n            double deltaY = curr.y - prev.y;\n            \n            // Filter outliers (large displacements)\n            if (Math.abs(deltaX) < 50 && Math.abs(deltaY) < 50) {\n                avgDeltaX += deltaX;\n                avgDeltaY += deltaY;\n                validMatches++;\n            }\n        }\n        \n        if (validMatches > 0) {\n            avgDeltaX /= validMatches;\n            avgDeltaY /= validMatches;\n            \n            // Convert pixel displacement to robot motion\n            // This is a simplified conversion - real implementation would use proper camera model\n            double scaleFactor = CAMERA_HEIGHT / FOCAL_LENGTH;\n            \n            estimatedX += avgDeltaX * scaleFactor * PIXEL_SIZE;\n            estimatedY += avgDeltaY * scaleFactor * PIXEL_SIZE;\n            \n            // Estimate rotation from feature displacement pattern\n            // This is a simplified approach - real implementation would use more sophisticated methods\n            estimatedTheta += Math.atan2(avgDeltaY, avgDeltaX) * 0.1; // Damping factor\n        }\n    }\n    \n    private void drawTrackingResults(Mat output) {\n        if (previousFeatures != null && currentFeatures != null) {\n            // Draw feature tracks\n            for (int i = 0; i < Math.min(previousFeatures.size(), currentFeatures.size()); i++) {\n                Point2f prev = previousFeatures.get(i);\n                Point2f curr = currentFeatures.get(i);\n                \n                Imgproc.line(output, new Point(prev.x, prev.y), \n                    new Point(curr.x, curr.y), new Scalar(0, 255, 0), 1);\n                Imgproc.circle(output, new Point(curr.x, curr.y), 3, \n                    new Scalar(255, 0, 0), -1);\n            }\n        }\n        \n        // Draw motion estimate\n        String motionText = String.format(\"Motion: X=%.1f, Y=%.1f, θ=%.2f\", \n            estimatedX, estimatedY, Math.toDegrees(estimatedTheta));\n        Imgproc.putText(output, motionText, new Point(10, 30), \n            Imgproc.FONT_HERSHEY_SIMPLEX, 0.7, new Scalar(0, 255, 255), 2);\n    }\n    \n    // Getter methods for accessing results\n    public double getEstimatedX() { return estimatedX; }\n    public double getEstimatedY() { return estimatedY; }\n    public double getEstimatedTheta() { return estimatedTheta; }\n    \n    public void resetOdometry() {\n        estimatedX = 0;\n        estimatedY = 0;\n        estimatedTheta = 0;\n    }\n}"
    },
    {
      "type": "text",
      "title": "Landmark-Based Localization",
      "content": "Landmark-based localization uses known visual landmarks (like AprilTags) to determine the robot's absolute position on the field. This provides a reference point that can correct drift in other navigation systems.<br><br><strong>Landmark Types:</strong><br>• <strong>AprilTags:</strong> Fiducial markers with known positions<br>• <strong>Field Features:</strong> Distinctive field elements like goals or walls<br>• <strong>Natural Landmarks:</strong> Unique visual features in the environment<br>• <strong>Team Elements:</strong> Game-specific objects with known properties<br><br><strong>Localization Process:</strong><br>1. <strong>Landmark Detection:</strong> Identify and locate landmarks in the image<br>2. <strong>Pose Estimation:</strong> Calculate robot position relative to landmarks<br>3. <strong>Triangulation:</strong> Use multiple landmarks for better accuracy<br>4. <strong>Fusion:</strong> Combine with other sensor data"
    },
    {
      "type": "code",
      "title": "Landmark-Based Localization Example",
      "content": "Example of landmark-based localization using AprilTags:",
      "code": "public class LandmarkLocalizationOpMode extends LinearOpMode {\n    private VisionPortal visionPortal;\n    private AprilTagProcessor aprilTagProcessor;\n    \n    // Known landmark positions (field coordinates in inches)\n    private Map<Integer, Point2d> landmarkPositions;\n    \n    // Robot pose estimate\n    private Pose2d robotPose;\n    private double poseConfidence;\n    \n    @Override\n    public void runOpMode() {\n        // Initialize landmark positions\n        initializeLandmarks();\n        \n        // Initialize AprilTag processor\n        aprilTagProcessor = new AprilTagProcessor.Builder()\n            .setTagLibrary(AprilTagLibrary.getCurrentGameTagLibrary())\n            .build();\n        \n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(aprilTagProcessor)\n            .build();\n        \n        waitForStart();\n        \n        while (opModeIsActive()) {\n            List<AprilTagDetection> detections = aprilTagProcessor.getDetections();\n            updateLocalization(detections);\n            \n            // Display results\n            displayLocalizationResults();\n            \n            telemetry.update();\n        }\n    }\n    \n    private void initializeLandmarks() {\n        landmarkPositions = new HashMap<>();\n        \n        // Add known AprilTag positions (example for a hypothetical field)\n        // These would be the actual field coordinates of each tag\n        landmarkPositions.put(1, new Point2d(0, 0));      // Origin\n        landmarkPositions.put(2, new Point2d(72, 0));     // Right side\n        landmarkPositions.put(3, new Point2d(0, 72));     // Back side\n        landmarkPositions.put(4, new Point2d(72, 72));    // Corner\n        landmarkPositions.put(5, new Point2d(36, 0));     // Center front\n        landmarkPositions.put(6, new Point2d(36, 72));    // Center back\n    }\n    \n    private void updateLocalization(List<AprilTagDetection> detections) {\n        if (detections.isEmpty()) {\n            poseConfidence = 0.0;\n            return;\n        }\n        \n        // Collect valid landmark observations\n        List<LandmarkObservation> observations = new ArrayList<>();\n        \n        for (AprilTagDetection detection : detections) {\n            if (landmarkPositions.containsKey(detection.id)) {\n                Point2d landmarkPosition = landmarkPositions.get(detection.id);\n                \n                // Get relative position from AprilTag pose\n                double relativeX = detection.ftcPose.x;\n                double relativeY = detection.ftcPose.y;\n                double relativeTheta = detection.ftcPose.yaw;\n                \n                // Create observation\n                LandmarkObservation obs = new LandmarkObservation(\n                    detection.id,\n                    landmarkPosition,\n                    relativeX,\n                    relativeY,\n                    relativeTheta,\n                    detection.ftcPose.range\n                );\n                \n                observations.add(obs);\n            }\n        }\n        \n        // Estimate robot pose from observations\n        if (observations.size() >= 1) {\n            estimateRobotPose(observations);\n        }\n    }\n    \n    private void estimateRobotPose(List<LandmarkObservation> observations) {\n        if (observations.size() == 1) {\n            // Single landmark - estimate position but not orientation\n            LandmarkObservation obs = observations.get(0);\n            \n            // Calculate robot position relative to landmark\n            double robotX = obs.landmarkPosition.x - obs.relativeX;\n            double robotY = obs.landmarkPosition.y - obs.relativeY;\n            \n            robotPose = new Pose2d(robotX, robotY, 0);\n            poseConfidence = 0.5; // Lower confidence with single landmark\n            \n        } else if (observations.size() >= 2) {\n            // Multiple landmarks - use triangulation\n            robotPose = triangulatePose(observations);\n            poseConfidence = 0.8; // Higher confidence with multiple landmarks\n        }\n    }\n    \n    private Pose2d triangulatePose(List<LandmarkObservation> observations) {\n        // Simple weighted average approach\n        // In practice, you might use more sophisticated methods like RANSAC\n        \n        double totalWeight = 0;\n        double weightedX = 0;\n        double weightedY = 0;\n        double weightedTheta = 0;\n        \n        for (LandmarkObservation obs : observations) {\n            // Weight by inverse distance (closer landmarks are more reliable)\n            double weight = 1.0 / (obs.range + 1.0);\n            \n            // Calculate robot position relative to this landmark\n            double robotX = obs.landmarkPosition.x - obs.relativeX;\n            double robotY = obs.landmarkPosition.y - obs.relativeY;\n            \n            weightedX += robotX * weight;\n            weightedY += robotY * weight;\n            weightedTheta += obs.relativeTheta * weight;\n            totalWeight += weight;\n        }\n        \n        if (totalWeight > 0) {\n            return new Pose2d(\n                weightedX / totalWeight,\n                weightedY / totalWeight,\n                weightedTheta / totalWeight\n            );\n        }\n        \n        return new Pose2d(0, 0, 0);\n    }\n    \n    private void displayLocalizationResults() {\n        if (poseConfidence > 0) {\n            telemetry.addData(\"Robot Position\", \n                \"X: %.1f, Y: %.1f, θ: %.1f\", \n                robotPose.getX(), robotPose.getY(), Math.toDegrees(robotPose.getHeading()));\n            telemetry.addData(\"Pose Confidence\", \"%.2f\", poseConfidence);\n        } else {\n            telemetry.addData(\"Robot Position\", \"No landmarks detected\");\n        }\n    }\n}\n\nclass LandmarkObservation {\n    public int landmarkId;\n    public Point2d landmarkPosition;\n    public double relativeX;\n    public double relativeY;\n    public double relativeTheta;\n    public double range;\n    \n    public LandmarkObservation(int id, Point2d pos, double relX, double relY, \n                              double relTheta, double r) {\n        this.landmarkId = id;\n        this.landmarkPosition = pos;\n        this.relativeX = relX;\n        this.relativeY = relY;\n        this.relativeTheta = relTheta;\n        this.range = r;\n    }\n}"
    },
    {
      "type": "text",
      "title": "Obstacle Detection and Avoidance",
      "content": "Vision-based obstacle detection allows robots to identify and avoid dynamic obstacles that aren't accounted for in pre-planned paths. This is crucial for safe autonomous operation in dynamic environments.<br><br><strong>Obstacle Detection Methods:</strong><br>• <strong>Depth Estimation:</strong> Calculate distance to objects using stereo vision or structured light<br>• <strong>Motion Detection:</strong> Identify moving objects in the scene<br>• <strong>Object Classification:</strong> Use ML to identify specific types of obstacles<br>• <strong>Contour Analysis:</strong> Detect potential obstacles based on shape and size<br><br><strong>Obstacle Avoidance Strategies:</strong><br>• <strong>Path Modification:</strong> Adjust planned path to avoid obstacles<br>• <strong>Speed Reduction:</strong> Slow down when obstacles are detected<br>• <strong>Emergency Stop:</strong> Stop immediately for critical obstacles<br>• <strong>Alternative Routes:</strong> Plan new paths around obstacles"
    },
    {
      "type": "code",
      "title": "Obstacle Detection and Avoidance Example",
      "content": "Example of obstacle detection and avoidance system:",
      "code": "public class ObstacleDetectionPipeline extends OpenCvPipeline {\n    \n    // Obstacle detection parameters\n    private static final double MIN_OBSTACLE_HEIGHT = 50; // pixels\n    private static final double MAX_OBSTACLE_DISTANCE = 1000; // mm\n    private static final double SAFETY_MARGIN = 200; // mm\n    \n    // Detection results\n    private List<Obstacle> detectedObstacles;\n    private boolean obstacleDetected;\n    private double closestObstacleDistance;\n    \n    // Camera parameters for distance estimation\n    private static final double FOCAL_LENGTH = 578.272;\n    private static final double KNOWN_OBJECT_HEIGHT = 300; // mm (typical robot height)\n    \n    public ObstacleDetectionPipeline() {\n        detectedObstacles = new ArrayList<>();\n    }\n    \n    @Override\n    public Mat processFrame(Mat input) {\n        Mat output = input.clone();\n        \n        // Clear previous detections\n        detectedObstacles.clear();\n        obstacleDetected = false;\n        closestObstacleDistance = Double.MAX_VALUE;\n        \n        // Convert to different color spaces for detection\n        Mat hsv = new Mat();\n        Mat gray = new Mat();\n        \n        Imgproc.cvtColor(input, hsv, Imgproc.COLOR_RGB2HSV);\n        Imgproc.cvtColor(input, gray, Imgproc.COLOR_RGB2GRAY);\n        \n        // Detect potential obstacles using multiple methods\n        detectObstaclesByMotion(gray, output);\n        detectObstaclesByColor(hsv, output);\n        detectObstaclesByContours(gray, output);\n        \n        // Analyze detected obstacles\n        analyzeObstacles(output);\n        \n        // Draw results and warnings\n        drawObstacleResults(output);\n        \n        // Clean up\n        hsv.release();\n        gray.release();\n        \n        return output;\n    }\n    \n    private void detectObstaclesByMotion(Mat gray, Mat output) {\n        // This is a simplified motion detection\n        // In practice, you would compare with previous frames\n        \n        // Use edge detection to find potential moving objects\n        Mat edges = new Mat();\n        Imgproc.Canny(gray, edges, 50, 150);\n        \n        // Find contours that could be obstacles\n        List<MatOfPoint> contours = new ArrayList<>();\n        Mat hierarchy = new Mat();\n        Imgproc.findContours(edges, contours, hierarchy, \n            Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);\n        \n        for (MatOfPoint contour : contours) {\n            double area = Imgproc.contourArea(contour);\n            if (area > 1000) { // Minimum area threshold\n                Rect bbox = Imgproc.boundingRect(contour);\n                \n                // Estimate distance based on object size\n                double estimatedDistance = estimateDistance(bbox.height);\n                \n                if (estimatedDistance < MAX_OBSTACLE_DISTANCE) {\n                    Obstacle obstacle = new Obstacle(\n                        \"motion\", bbox, estimatedDistance, 0.7\n                    );\n                    detectedObstacles.add(obstacle);\n                }\n            }\n        }\n        \n        edges.release();\n        hierarchy.release();\n    }\n    \n    private void detectObstaclesByColor(Mat hsv, Mat output) {\n        // Detect obstacles based on color (e.g., other robots)\n        // This example detects red objects (other robots)\n        \n        Mat redMask = new Mat();\n        Mat redMask1 = new Mat();\n        Mat redMask2 = new Mat();\n        \n        // Red color ranges in HSV\n        Core.inRange(hsv, new Scalar(0, 100, 100), \n            new Scalar(10, 255, 255), redMask1);\n        Core.inRange(hsv, new Scalar(170, 100, 100), \n            new Scalar(180, 255, 255), redMask2);\n        Core.add(redMask1, redMask2, redMask);\n        \n        // Find red objects\n        List<MatOfPoint> contours = new ArrayList<>();\n        Mat hierarchy = new Mat();\n        Imgproc.findContours(redMask, contours, hierarchy, \n            Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);\n        \n        for (MatOfPoint contour : contours) {\n            double area = Imgproc.contourArea(contour);\n            if (area > 2000) { // Larger threshold for colored objects\n                Rect bbox = Imgproc.boundingRect(contour);\n                double estimatedDistance = estimateDistance(bbox.height);\n                \n                if (estimatedDistance < MAX_OBSTACLE_DISTANCE) {\n                    Obstacle obstacle = new Obstacle(\n                        \"red_robot\", bbox, estimatedDistance, 0.9\n                    );\n                    detectedObstacles.add(obstacle);\n                }\n            }\n        }\n        \n        redMask.release();\n        redMask1.release();\n        redMask2.release();\n        hierarchy.release();\n    }\n    \n    private void detectObstaclesByContours(Mat gray, Mat output) {\n        // Detect obstacles based on shape and size\n        \n        // Apply threshold to find dark objects\n        Mat thresh = new Mat();\n        Imgproc.threshold(gray, thresh, 100, 255, Imgproc.THRESH_BINARY_INV);\n        \n        // Apply morphological operations\n        Mat kernel = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(3, 3));\n        Mat morphed = new Mat();\n        Imgproc.morphologyEx(thresh, morphed, Imgproc.MORPH_CLOSE, kernel);\n        \n        // Find contours\n        List<MatOfPoint> contours = new ArrayList<>();\n        Mat hierarchy = new Mat();\n        Imgproc.findContours(morphed, contours, hierarchy, \n            Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);\n        \n        for (MatOfPoint contour : contours) {\n            double area = Imgproc.contourArea(contour);\n            if (area > 1500) {\n                Rect bbox = Imgproc.boundingRect(contour);\n                \n                // Check if object is tall enough to be an obstacle\n                if (bbox.height > MIN_OBSTACLE_HEIGHT) {\n                    double estimatedDistance = estimateDistance(bbox.height);\n                    \n                    if (estimatedDistance < MAX_OBSTACLE_DISTANCE) {\n                        Obstacle obstacle = new Obstacle(\n                            \"dark_object\", bbox, estimatedDistance, 0.6\n                        );\n                        detectedObstacles.add(obstacle);\n                    }\n                }\n            }\n        }\n        \n        thresh.release();\n        morphed.release();\n        kernel.release();\n        hierarchy.release();\n    }\n    \n    private double estimateDistance(double objectHeightPixels) {\n        // Simple distance estimation using known object height\n        // Distance = (known_height * focal_length) / pixel_height\n        return (KNOWN_OBJECT_HEIGHT * FOCAL_LENGTH) / objectHeightPixels;\n    }\n    \n    private void analyzeObstacles(Mat output) {\n        if (detectedObstacles.isEmpty()) {\n            return;\n        }\n        \n        // Find closest obstacle\n        Obstacle closest = detectedObstacles.get(0);\n        for (Obstacle obstacle : detectedObstacles) {\n            if (obstacle.distance < closest.distance) {\n                closest = obstacle;\n            }\n        }\n        \n        closestObstacleDistance = closest.distance;\n        \n        // Check if obstacle is too close\n        if (closest.distance < SAFETY_MARGIN) {\n            obstacleDetected = true;\n        }\n        \n        // Determine avoidance strategy\n        determineAvoidanceStrategy(closest);\n    }\n    \n    private void determineAvoidanceStrategy(Obstacle closest) {\n        if (closest.distance < SAFETY_MARGIN / 2) {\n            // Emergency stop needed\n            telemetry.addData(\"Obstacle Warning\", \"EMERGENCY STOP - Obstacle too close!\");\n        } else if (closest.distance < SAFETY_MARGIN) {\n            // Slow down and plan avoidance\n            telemetry.addData(\"Obstacle Warning\", \"Slow down - Obstacle detected\");\n        } else {\n            // Normal operation with caution\n            telemetry.addData(\"Obstacle Status\", \"Clear - Obstacle at safe distance\");\n        }\n    }\n    \n    private void drawObstacleResults(Mat output) {\n        // Draw detected obstacles\n        for (Obstacle obstacle : detectedObstacles) {\n            Scalar color;\n            if (obstacle.distance < SAFETY_MARGIN) {\n                color = new Scalar(0, 0, 255); // Red for close obstacles\n            } else {\n                color = new Scalar(0, 255, 0); // Green for safe obstacles\n            }\n            \n            Imgproc.rectangle(output, obstacle.boundingBox, color, 2);\n            \n            // Add distance label\n            Point textPoint = new Point(\n                obstacle.boundingBox.x, \n                obstacle.boundingBox.y - 10\n            );\n            Imgproc.putText(output, \n                String.format(\"%s %.0fmm\", obstacle.type, obstacle.distance),\n                textPoint, Imgproc.FONT_HERSHEY_SIMPLEX, 0.5, color, 1);\n        }\n        \n        // Add overall status\n        String statusText = obstacleDetected ? \n            \"OBSTACLE DETECTED\" : \"CLEAR\";\n        Scalar statusColor = obstacleDetected ? \n            new Scalar(0, 0, 255) : new Scalar(0, 255, 0);\n        \n        Imgproc.putText(output, statusText, new Point(10, 30), \n            Imgproc.FONT_HERSHEY_SIMPLEX, 1.0, statusColor, 2);\n    }\n    \n    // Getter methods\n    public boolean isObstacleDetected() { return obstacleDetected; }\n    public double getClosestObstacleDistance() { return closestObstacleDistance; }\n    public List<Obstacle> getDetectedObstacles() { return detectedObstacles; }\n}\n\nclass Obstacle {\n    public String type;\n    public Rect boundingBox;\n    public double distance;\n    public double confidence;\n    \n    public Obstacle(String type, Rect bbox, double dist, double conf) {\n        this.type = type;\n        this.boundingBox = bbox;\n        this.distance = dist;\n        this.confidence = conf;\n    }\n}"
    },
    {
      "type": "text",
      "title": "Vision-Enhanced Path Following",
      "content": "Vision can significantly improve the accuracy of path following by providing visual feedback that compensates for errors in traditional navigation systems.<br><br><strong>Vision Feedback Integration:</strong><br>• <strong>Visual Path Tracking:</strong> Follow visual markers or lines on the field<br>• <strong>Landmark Correction:</strong> Use landmarks to correct position estimates<br>• <strong>Visual Servoing:</strong> Control robot motion based on visual feedback<br>• <strong>Error Correction:</strong> Compensate for encoder slip and wheel wear<br><br><strong>Integration with Path Planning:</strong><br>Vision-enhanced path following works best when integrated with existing path planning systems like RoadRunner or PedroPathing. The vision system provides corrections and adjustments to the planned path based on real-time visual feedback."
    },
    {
      "type": "exercise-box",
      "title": "Vision Navigation System Exercise",
      "description": "Create a complete vision-based navigation system for your robot",
      "tasks": [
        "Implement visual odometry using feature tracking",
        "Add landmark-based localization using AprilTags",
        "Create obstacle detection and avoidance system",
        "Integrate vision feedback with your path following system",
        "Test the complete system in various field conditions"
      ],
      "content": "Build a comprehensive vision-based navigation system that combines visual odometry, landmark localization, and obstacle detection. The system should provide accurate positioning and safe navigation in dynamic environments."
    },
    {
      "type": "link-grid",
      "title": "Additional Resources",
      "links": [
        "<a href=\"https://gm0.org/en/latest/docs/software/tutorials/odometry.html#visual-odometry\" target=\"_blank\">gm0: Visual Odometry</a>",
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/programming_resources/vision/apriltag.html#localization\" target=\"_blank\">FTC Localization</a>",
        "<a href=\"https://gm0.org/en/latest/docs/software/tutorials/vision.html#obstacle-detection\" target=\"_blank\">gm0: Obstacle Detection</a>",
        "<a href=\"https://learnroadrunner.com/\" target=\"_blank\">RoadRunner Documentation</a>"
      ]
    }
  ]
} 