{
  "title": "TensorFlow Lite",
  "sections": [
    {
      "type": "text",
      "title": "Machine Learning in FTC",
      "content": "Machine learning (ML) enables robots to perform complex recognition and classification tasks that would be difficult or impossible with traditional computer vision techniques. TensorFlow Lite brings powerful ML capabilities to FTC robots, allowing them to recognize objects, classify images, and make intelligent decisions based on visual data.<br><br>ML in FTC opens up new possibilities for autonomous operation, including object detection, gesture recognition, and adaptive behavior based on visual feedback."
    },
    {
      "type": "rules-box",
      "title": "ML Applications in FTC",
      "items": [
        "Object detection and classification for game elements",
        "Gesture recognition for driver interaction",
        "Quality assessment of robot operations",
        "Adaptive behavior based on field conditions",
        "Predictive maintenance and fault detection",
        "Advanced navigation and obstacle avoidance"
      ]
    },
    {
      "type": "text",
      "title": "TensorFlow Lite Overview",
      "content": "TensorFlow Lite is a lightweight version of TensorFlow designed for mobile and embedded devices. It's optimized for low-power, low-memory environments like the FTC Control Hub.<br><br><strong>Key Features:</strong><br>• <strong>Model Optimization:</strong> Quantized models for faster inference<br>• <strong>Cross-Platform:</strong> Works on Android, iOS, and embedded systems<br>• <strong>Pre-trained Models:</strong> Access to models for common tasks<br>• <strong>Custom Training:</strong> Support for training custom models<br>• <strong>Real-time Performance:</strong> Optimized for live inference<br><br><strong>FTC Integration:</strong><br>TensorFlow Lite is integrated into the FTC SDK through the TfodProcessor, making it easy to add ML capabilities to your robot."
    },
    {
      "type": "code",
      "title": "Basic TensorFlow Lite Setup",
      "content": "Setting up TensorFlow Lite in FTC is straightforward:",
      "code": "public class TensorFlowLiteOpMode extends LinearOpMode {\n    private VisionPortal visionPortal;\n    private TfodProcessor tfodProcessor;\n    \n    @Override\n    public void runOpMode() {\n        // Initialize TensorFlow Lite processor\n        tfodProcessor = new TfodProcessor.Builder()\n            .setUseObjectTracker(true)\n            .setMaxNumDetections(10)\n            .build();\n        \n        // Load a pre-trained model\n        // You can use built-in models or custom models\n        tfodProcessor.setModelFromAsset(\"model.tflite\");\n        tfodProcessor.setLabelsFromAsset(\"labels.txt\");\n        \n        // Build Vision Portal\n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(tfodProcessor)\n            .setCameraResolution(new Size(640, 480))\n            .setStreamFormat(VisionPortal.StreamFormat.YUY2)\n            .enableLiveView(true)\n            .enableCameraMonitoring(true)\n            .build();\n        \n        waitForStart();\n        \n        while (opModeIsActive()) {\n            // Get TensorFlow Lite detections\n            List<Recognition> recognitions = tfodProcessor.getRecognitions();\n            \n            // Process detections\n            processTensorFlowDetections(recognitions);\n            \n            telemetry.update();\n        }\n    }\n    \n    private void processTensorFlowDetections(List<Recognition> recognitions) {\n        if (recognitions.isEmpty()) {\n            telemetry.addData(\"TensorFlow\", \"No objects detected\");\n            return;\n        }\n        \n        for (Recognition recognition : recognitions) {\n            // Get detection information\n            String label = recognition.getLabel();\n            float confidence = recognition.getConfidence();\n            RectF boundingBox = recognition.getLocation();\n            \n            // Display results\n            telemetry.addData(\"Object\", label);\n            telemetry.addData(\"Confidence\", \"%.2f\", confidence);\n            telemetry.addData(\"Location\", \"(%.0f, %.0f, %.0f, %.0f)\", \n                boundingBox.left, boundingBox.top, \n                boundingBox.right, boundingBox.bottom);\n            \n            // Calculate object center\n            float centerX = (boundingBox.left + boundingBox.right) / 2;\n            float centerY = (boundingBox.top + boundingBox.bottom) / 2;\n            telemetry.addData(\"Center\", \"(%.0f, %.0f)\", centerX, centerY);\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "TensorFlow Lite Model Integration",
      "content": "Integrating ML models into your FTC robot involves several steps:<br><br><strong>Model Selection:</strong><br>• <strong>Pre-trained Models:</strong> Use existing models for common tasks<br>• <strong>Custom Models:</strong> Train models specific to your needs<br>• <strong>Model Format:</strong> Ensure models are in TensorFlow Lite format<br><br><strong>Integration Process:</strong><br>1. <strong>Model Preparation:</strong> Convert and optimize your model<br>2. <strong>Asset Management:</strong> Add model files to your project<br>3. <strong>Processor Configuration:</strong> Set up TfodProcessor<br>4. <strong>Label Management:</strong> Define class labels for your model<br>5. <strong>Testing and Validation:</strong> Verify model performance"
    },
    {
      "type": "code",
      "title": "Advanced TensorFlow Lite Configuration",
      "content": "Advanced configuration options for TensorFlow Lite:",
      "code": "public class AdvancedTensorFlowOpMode extends LinearOpMode {\n    private VisionPortal visionPortal;\n    private TfodProcessor tfodProcessor;\n    \n    // Configuration parameters\n    private static final float MIN_CONFIDENCE = 0.7f;\n    private static final int MAX_DETECTIONS = 5;\n    private static final boolean USE_OBJECT_TRACKER = true;\n    \n    @Override\n    public void runOpMode() {\n        // Advanced TensorFlow Lite configuration\n        tfodProcessor = new TfodProcessor.Builder()\n            .setUseObjectTracker(USE_OBJECT_TRACKER)\n            .setMaxNumDetections(MAX_DETECTIONS)\n            .setTrackerMaxOverlap(0.2f)\n            .setTrackerMinSize(16)\n            .setTrackerMarginalCorrelation(0.75f)\n            .setTrackerMinCorrelation(0.75f)\n            .build();\n        \n        // Load custom model\n        tfodProcessor.setModelFromAsset(\"custom_model.tflite\");\n        tfodProcessor.setLabelsFromAsset(\"custom_labels.txt\");\n        \n        // Set minimum confidence threshold\n        tfodProcessor.setMinResultConfidence(MIN_CONFIDENCE);\n        \n        // Advanced Vision Portal configuration\n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(tfodProcessor)\n            .setCameraResolution(new Size(1280, 720))\n            .setStreamFormat(VisionPortal.StreamFormat.YUY2)\n            .enableLiveView(true)\n            .enableCameraMonitoring(true)\n            .build();\n        \n        waitForStart();\n        \n        while (opModeIsActive()) {\n            List<Recognition> recognitions = tfodProcessor.getRecognitions();\n            processAdvancedDetections(recognitions);\n            telemetry.update();\n        }\n    }\n    \n    private void processAdvancedDetections(List<Recognition> recognitions) {\n        // Filter by confidence\n        List<Recognition> highConfidenceDetections = recognitions.stream()\n            .filter(recognition -> recognition.getConfidence() >= MIN_CONFIDENCE)\n            .collect(Collectors.toList());\n        \n        if (highConfidenceDetections.isEmpty()) {\n            telemetry.addData(\"Status\", \"No high-confidence detections\");\n            return;\n        }\n        \n        // Sort by confidence\n        highConfidenceDetections.sort((a, b) -> \n            Float.compare(b.getConfidence(), a.getConfidence()));\n        \n        for (Recognition recognition : highConfidenceDetections) {\n            // Get tracking ID if using object tracker\n            Integer trackingId = recognition.getTrackingId();\n            \n            telemetry.addData(\"Detection\", \n                \"ID: %s, Label: %s, Confidence: %.2f\", \n                trackingId != null ? trackingId.toString() : \"N/A\",\n                recognition.getLabel(), \n                recognition.getConfidence());\n            \n            // Calculate object properties\n            RectF bbox = recognition.getLocation();\n            float area = (bbox.right - bbox.left) * (bbox.bottom - bbox.top);\n            telemetry.addData(\"Object Area\", \"%.0f pixels\", area);\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Custom Model Training and Conversion",
      "content": "While pre-trained models are useful, custom models trained on your specific data often provide better performance for FTC applications.<br><br><strong>Training Process:</strong><br>• <strong>Data Collection:</strong> Gather images of objects you want to detect<br>• <strong>Data Annotation:</strong> Label images with bounding boxes and classes<br>• <strong>Model Training:</strong> Train using TensorFlow or similar frameworks<br>• <strong>Model Conversion:</strong> Convert to TensorFlow Lite format<br>• <strong>Optimization:</strong> Quantize and optimize for mobile deployment<br><br><strong>Training Considerations:</strong><br>• <strong>Data Quality:</strong> Ensure diverse, representative training data<br>• <strong>Data Augmentation:</strong> Use techniques to increase dataset size<br>• <strong>Model Architecture:</strong> Choose appropriate model size for FTC hardware<br>• <strong>Training Time:</strong> Balance training time with model performance"
    },
    {
      "type": "code",
      "title": "Custom Model Integration Example",
      "content": "Example of integrating a custom-trained model:",
      "code": "public class CustomModelOpMode extends LinearOpMode {\n    private VisionPortal visionPortal;\n    private TfodProcessor tfodProcessor;\n    \n    // Custom model configuration\n    private static final String MODEL_ASSET = \"ftc_game_elements.tflite\";\n    private static final String LABELS_ASSET = \"ftc_game_labels.txt\";\n    private static final float MIN_CONFIDENCE = 0.6f;\n    \n    // Game-specific object classes\n    private enum GameElement {\n        RED_CONE(\"red_cone\"),\n        BLUE_CONE(\"blue_cone\"),\n        YELLOW_CONE(\"yellow_cone\"),\n        RED_POLE(\"red_pole\"),\n        BLUE_POLE(\"blue_pole\"),\n        YELLOW_POLE(\"yellow_pole\");\n        \n        private final String label;\n        \n        GameElement(String label) {\n            this.label = label;\n        }\n        \n        public String getLabel() {\n            return label;\n        }\n    }\n    \n    @Override\n    public void runOpMode() {\n        // Configure custom model\n        tfodProcessor = new TfodProcessor.Builder()\n            .setUseObjectTracker(true)\n            .setMaxNumDetections(10)\n            .setMinResultConfidence(MIN_CONFIDENCE)\n            .build();\n        \n        // Load custom model and labels\n        tfodProcessor.setModelFromAsset(MODEL_ASSET);\n        tfodProcessor.setLabelsFromAsset(LABELS_ASSET);\n        \n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(tfodProcessor)\n            .setCameraResolution(new Size(640, 480))\n            .build();\n        \n        waitForStart();\n        \n        while (opModeIsActive()) {\n            List<Recognition> recognitions = tfodProcessor.getRecognitions();\n            processGameElements(recognitions);\n            telemetry.update();\n        }\n    }\n    \n    private void processGameElements(List<Recognition> recognitions) {\n        // Group detections by type\n        Map<GameElement, List<Recognition>> elementGroups = new HashMap<>();\n        \n        for (Recognition recognition : recognitions) {\n            String label = recognition.getLabel();\n            \n            // Find matching game element\n            for (GameElement element : GameElement.values()) {\n                if (element.getLabel().equals(label)) {\n                    elementGroups.computeIfAbsent(element, k -> new ArrayList<>())\n                        .add(recognition);\n                    break;\n                }\n            }\n        }\n        \n        // Process each element type\n        for (Map.Entry<GameElement, List<Recognition>> entry : elementGroups.entrySet()) {\n            GameElement element = entry.getKey();\n            List<Recognition> detections = entry.getValue();\n            \n            telemetry.addData(\"%s Count\", element.name(), detections.size());\n            \n            // Find closest detection of this type\n            Recognition closest = findClosestDetection(detections);\n            if (closest != null) {\n                RectF bbox = closest.getLocation();\n                float centerX = (bbox.left + bbox.right) / 2;\n                float centerY = (bbox.top + bbox.bottom) / 2;\n                \n                telemetry.addData(\"Closest %s\", \n                    \"Center: (%.0f, %.0f), Confidence: %.2f\", \n                    element.name(), centerX, centerY, closest.getConfidence());\n            }\n        }\n    }\n    \n    private Recognition findClosestDetection(List<Recognition> detections) {\n        if (detections.isEmpty()) {\n            return null;\n        }\n        \n        // Find detection closest to image center (320, 240 for 640x480)\n        Recognition closest = detections.get(0);\n        float minDistance = Float.MAX_VALUE;\n        \n        for (Recognition detection : detections) {\n            RectF bbox = detection.getLocation();\n            float centerX = (bbox.left + bbox.right) / 2;\n            float centerY = (bbox.top + bbox.bottom) / 2;\n            \n            float distance = (float) Math.sqrt(\n                Math.pow(centerX - 320, 2) + Math.pow(centerY - 240, 2));\n            \n            if (distance < minDistance) {\n                minDistance = distance;\n                closest = detection;\n            }\n        }\n        \n        return closest;\n    }\n}"
    },
    {
      "type": "text",
      "title": "Real-time Inference Optimization",
      "content": "ML inference must be fast enough for real-time robot control. Several optimization techniques can improve performance:<br><br><strong>Model Optimization:</strong><br>• <strong>Quantization:</strong> Reduce model precision to improve speed<br>• <strong>Pruning:</strong> Remove unnecessary model parameters<br>• <strong>Model Compression:</strong> Use smaller, more efficient architectures<br><br><strong>Runtime Optimization:</strong><br>• <strong>Thread Management:</strong> Use appropriate thread pools<br>• <strong>Memory Management:</strong> Efficient memory allocation and cleanup<br>• <strong>Batch Processing:</strong> Process multiple inputs together<br>• <strong>GPU Acceleration:</strong> Use hardware acceleration when available"
    },
    {
      "type": "code",
      "title": "Performance Optimization Example",
      "content": "Example of optimized TensorFlow Lite implementation:",
      "code": "public class OptimizedTensorFlowOpMode extends LinearOpMode {\n    private VisionPortal visionPortal;\n    private TfodProcessor tfodProcessor;\n    \n    // Performance monitoring\n    private long lastInferenceTime = 0;\n    private double averageInferenceTime = 0;\n    private int frameCount = 0;\n    \n    // Optimization parameters\n    private static final int PROCESS_EVERY_N_FRAMES = 3; // Process every 3rd frame\n    private static final float MIN_CONFIDENCE = 0.8f;\n    private static final int MAX_DETECTIONS = 3;\n    \n    @Override\n    public void runOpMode() {\n        // Optimized TensorFlow Lite configuration\n        tfodProcessor = new TfodProcessor.Builder()\n            .setUseObjectTracker(true)\n            .setMaxNumDetections(MAX_DETECTIONS)\n            .setMinResultConfidence(MIN_CONFIDENCE)\n            .setNumInterpreterThreads(2) // Use 2 threads for inference\n            .build();\n        \n        // Load optimized model\n        tfodProcessor.setModelFromAsset(\"optimized_model.tflite\");\n        tfodProcessor.setLabelsFromAsset(\"labels.txt\");\n        \n        // Configure Vision Portal for performance\n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(tfodProcessor)\n            .setCameraResolution(new Size(640, 480)) // Lower resolution for speed\n            .setStreamFormat(VisionPortal.StreamFormat.YUY2)\n            .enableLiveView(false) // Disable live view for performance\n            .enableCameraMonitoring(false)\n            .build();\n        \n        waitForStart();\n        \n        while (opModeIsActive()) {\n            long startTime = System.nanoTime();\n            \n            // Only process every N frames\n            if (frameCount % PROCESS_EVERY_N_FRAMES == 0) {\n                List<Recognition> recognitions = tfodProcessor.getRecognitions();\n                processOptimizedDetections(recognitions);\n                \n                // Calculate inference time\n                long inferenceTime = System.nanoTime() - startTime;\n                updatePerformanceMetrics(inferenceTime);\n            }\n            \n            frameCount++;\n            telemetry.update();\n        }\n    }\n    \n    private void processOptimizedDetections(List<Recognition> recognitions) {\n        // Quick processing - only handle high-confidence detections\n        for (Recognition recognition : recognitions) {\n            if (recognition.getConfidence() >= MIN_CONFIDENCE) {\n                // Simple decision making based on detection\n                String label = recognition.getLabel();\n                RectF bbox = recognition.getLocation();\n                \n                // Calculate object center for robot control\n                float centerX = (bbox.left + bbox.right) / 2;\n                float centerY = (bbox.top + bbox.bottom) / 2;\n                \n                // Simple robot control logic\n                if (label.equals(\"target_object\")) {\n                    // Move robot toward detected object\n                    moveTowardObject(centerX, centerY);\n                }\n                \n                // Minimal telemetry for performance\n                telemetry.addData(\"Detected\", \"%s at (%.0f, %.0f)\", label, centerX, centerY);\n            }\n        }\n    }\n    \n    private void updatePerformanceMetrics(long inferenceTime) {\n        // Update running average\n        averageInferenceTime = (averageInferenceTime * 0.9) + \n            (inferenceTime / 1_000_000.0 * 0.1); // Convert to milliseconds\n        \n        // Display performance metrics\n        telemetry.addData(\"Inference Time\", \"%.2f ms\", averageInferenceTime);\n        telemetry.addData(\"FPS\", \"%.1f\", 1000.0 / averageInferenceTime);\n        telemetry.addData(\"Frame Count\", frameCount);\n    }\n    \n    private void moveTowardObject(float centerX, float centerY) {\n        // Simple robot control logic\n        // This would integrate with your robot's drive system\n        float errorX = centerX - 320; // Center of 640x480 image\n        float errorY = centerY - 240;\n        \n        // Convert to robot movement commands\n        // Implementation depends on your robot's drive system\n    }\n}"
    },
    {
      "type": "exercise-box",
      "title": "Custom Object Detection Exercise",
      "description": "Create a custom object detection system for FTC game elements",
      "tasks": [
        "Collect training data for specific game elements",
        "Train a custom TensorFlow Lite model",
        "Integrate the model into your FTC robot code",
        "Implement real-time object detection and tracking",
        "Optimize the system for performance and reliability"
      ],
      "content": "Build a complete custom object detection system using TensorFlow Lite. The system should detect specific game elements relevant to your FTC game and provide reliable, real-time results."
    },
    {
      "type": "link-grid",
      "title": "Additional Resources",
      "links": [
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/programming_resources/vision/tensorflow.html\" target=\"_blank\">FTC TensorFlow Documentation</a>",
        "<a href=\"https://www.tensorflow.org/lite/guide/android\" target=\"_blank\">TensorFlow Lite Android Guide</a>",
        "<a href=\"https://www.tensorflow.org/lite/models/modify/model_maker\" target=\"_blank\">TensorFlow Model Maker</a>",
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/programming_resources/vision/tensorflow.html#performance\" target=\"_blank\">FTC TensorFlow Performance</a>"
      ]
    }
  ]
} 