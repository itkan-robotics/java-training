{
  "title": "Vision-Based Navigation",
  "sections": [
    {
      "type": "text",
      "title": "What is Vision-Based Navigation?",
      "content": "Vision-based navigation combines computer vision with robot control to enable autonomous movement and positioning. Unlike traditional navigation methods that rely solely on encoders or IMU data, vision-based navigation uses visual information to determine the robot's position, plan paths, and execute precise movements.<br><br>Vision-based navigation is particularly powerful in FTC because it:<br>• Provides absolute positioning information using visual landmarks<br>• Enables precise alignment and targeting for game elements<br>• Allows dynamic path planning based on visual feedback<br>• Supports obstacle detection and avoidance<br>• Enables complex autonomous behaviors that would be difficult with other sensors"
    },
    {
      "type": "rules-box",
      "title": "Vision-Based Navigation Advantages",
      "items": [
        "Absolute positioning using visual landmarks",
        "Precise alignment and targeting capabilities",
        "Dynamic obstacle detection and avoidance",
        "Real-time path planning and adjustment",
        "Integration with other sensor data for robustness",
        "Support for complex autonomous behaviors"
      ]
    },
    {
      "type": "text",
      "title": "Vision-Based Localization",
      "content": "Vision-based localization determines the robot's position and orientation in the world using visual information. This is fundamental to vision-based navigation and provides the foundation for all other navigation tasks.<br><br><strong>Landmark-Based Localization:</strong> Using known visual landmarks (like AprilTags) to determine position<br><strong>Visual Odometry:</strong> Estimating movement by tracking visual features between frames<br><strong>Pose Estimation:</strong> Determining 6-degree-of-freedom position and orientation<br><strong>Coordinate Systems:</strong> Converting between camera, robot, and world coordinate frames<br><strong>Localization Accuracy:</strong> Factors affecting positioning precision and reliability"
    },
    {
      "type": "text",
      "title": "Vision-Based Localization System Setup",
      "content": "Let's create a vision-based localization system using AprilTags. For more on AprilTag localization, see <a href=\"https://ftc-docs.firstinspires.org/en/latest/apriltag/vision_portal/apriltag_intro/apriltag-intro.html\" target=\"_blank\">FTC AprilTag Documentation</a>."
    },
    {
      "type": "code",
      "title": "Import Statements and Class Definition",
      "content": "Import necessary AprilTag classes and define the localization system class.",
      "code": "import org.firstinspires.ftc.vision.VisionPortal;\nimport org.firstinspires.ftc.vision.apriltag.AprilTagProcessor;\nimport org.firstinspires.ftc.vision.apriltag.AprilTagDetection;\nimport org.firstinspires.ftc.vision.apriltag.AprilTagPoseFtc;\nimport java.util.List;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class VisionLocalizationSystem {\n    private VisionPortal visionPortal;\n    private AprilTagProcessor aprilTagProcessor;\n    \n    // Robot pose in world coordinates\n    private double robotX = 0.0;\n    private double robotY = 0.0;\n    private double robotHeading = 0.0;\n    \n    // Known AprilTag positions in world coordinates (inches)\n    private Map<Integer, Point2D> tagPositions = new HashMap<>();\n    \n    // Localization parameters\n    private static final double LOCALIZATION_UPDATE_RATE = 30.0; // Hz\n    private static final double MIN_TAG_DISTANCE = 6.0; // inches\n    private static final double MAX_TAG_DISTANCE = 120.0; // inches\n    // ..."
    },
    {
      "type": "text",
      "title": "System Initialization",
      "content": "Initialize the localization system with known tag positions and AprilTag processor. For more on AprilTag setup, see <a href=\"https://ftc-docs.firstinspires.org/en/latest/apriltag/vision_portal/apriltag_intro/apriltag-intro.html#setup\" target=\"_blank\">FTC AprilTag Setup</a>."
    },
    {
      "type": "code",
      "title": "Constructor and Initialization",
      "content": "Initialize the localization system with tag positions and AprilTag processor.",
      "code": "    public VisionLocalizationSystem() {\n        // Initialize known tag positions (example for a specific field)\n        initializeTagPositions();\n        \n        // Initialize AprilTag processor\n        aprilTagProcessor = new AprilTagProcessor.Builder()\n            .setTagLibrary(AprilTagLibrary.getCurrentGameTagLibrary())\n            .build();\n        \n        // Build Vision Portal\n        visionPortal = new VisionPortal.Builder()\n            .setCamera(hardwareMap.get(WebcamName.class, \"Webcam 1\"))\n            .addProcessor(aprilTagProcessor)\n            .setCameraResolution(new Size(640, 480))\n            .build();\n    }"
    },
    {
      "type": "text",
      "title": "Tag Position Initialization",
      "content": "Initialize known AprilTag positions in world coordinates for localization reference."
    },
    {
      "type": "code",
      "title": "initializeTagPositions Method",
      "content": "Set up known AprilTag positions for the specific field layout.",
      "code": "    private void initializeTagPositions() {\n        // Example tag positions for a specific field layout\n        // These would be measured and calibrated for your specific field\n        tagPositions.put(1, new Point2D(0, 0));      // Origin\n        tagPositions.put(2, new Point2D(24, 0));     // 24 inches right\n        tagPositions.put(3, new Point2D(48, 0));     // 48 inches right\n        tagPositions.put(4, new Point2D(0, 24));     // 24 inches forward\n        tagPositions.put(5, new Point2D(24, 24));    // Diagonal\n        tagPositions.put(6, new Point2D(48, 24));    // Far right, forward\n    }"
    },
    {
      "type": "text",
      "title": "Localization Update",
      "content": "The updateLocalization method processes AprilTag detections to update the robot's position and orientation."
    },
    {
      "type": "code",
      "title": "updateLocalization Method",
      "content": "Update robot localization based on AprilTag detections.",
      "code": "    public void updateLocalization() {\n        List<AprilTagDetection> detections = aprilTagProcessor.getDetections();\n        \n        if (detections.isEmpty()) {\n            // No tags detected - could use dead reckoning or other sensors\n            return;\n        }\n        \n        // Find the best tag for localization\n        AprilTagDetection bestTag = findBestLocalizationTag(detections);\n        \n        if (bestTag != null) {\n            // Update robot pose based on tag detection\n            updateRobotPose(bestTag);\n        }\n    }"
    },
    {
      "type": "text",
      "title": "Best Tag Selection",
      "content": "The findBestLocalizationTag method selects the most reliable AprilTag for localization based on distance and angle."
    },
    {
      "type": "code",
      "title": "findBestLocalizationTag Method",
      "content": "Find the best AprilTag for localization based on distance and reliability.",
      "code": "    private AprilTagDetection findBestLocalizationTag(List<AprilTagDetection> detections) {\n        AprilTagDetection bestTag = null;\n        double bestScore = 0.0;\n        \n        for (AprilTagDetection detection : detections) {\n            // Check if we know this tag's position\n            if (!tagPositions.containsKey(detection.id)) {\n                continue;\n            }\n            \n            // Calculate score based on distance and reliability\n            double distance = detection.ftcPose.z;\n            if (distance < MIN_TAG_DISTANCE || distance > MAX_TAG_DISTANCE) {\n                continue;\n            }\n            \n            // Score based on distance (closer is better) and angle (facing tag is better)\n            double angleScore = Math.cos(Math.abs(detection.ftcPose.yaw));\n            double distanceScore = 1.0 / (1.0 + distance / 12.0); // Normalize distance\n            double score = angleScore * distanceScore;\n            \n            if (score > bestScore) {\n                bestScore = score;\n                bestTag = detection;\n            }\n        }\n        \n        return bestTag;\n    }"
    },
    {
      "type": "text",
      "title": "Robot Pose Update",
      "content": "The updateRobotPose method converts AprilTag-relative pose information to world coordinates."
    },
    {
      "type": "code",
      "title": "updateRobotPose Method",
      "content": "Update robot pose in world coordinates based on AprilTag detection.",
      "code": "    private void updateRobotPose(AprilTagDetection tag) {\n        AprilTagPoseFtc tagPose = tag.ftcPose;\n        Point2D tagWorldPos = tagPositions.get(tag.id);\n        \n        // Convert tag-relative pose to world coordinates\n        // This is a simplified transformation - you might need more complex math\n        double tagDistance = tagPose.z;\n        double tagAngle = tagPose.yaw;\n        double tagXOffset = tagPose.x;\n        double tagYOffset = tagPose.y;\n        \n        // Calculate robot position in world coordinates\n        // This assumes the robot is facing the tag\n        robotX = tagWorldPos.x - tagXOffset;\n        robotY = tagWorldPos.y - tagYOffset;\n        robotHeading = tagAngle;\n        \n        // Normalize heading to -180 to 180 degrees\n        robotHeading = normalizeAngle(robotHeading);\n    }"
    },
    {
      "type": "text",
      "title": "Angle Normalization",
      "content": "The normalizeAngle method ensures angles are within the proper range for consistent calculations."
    },
    {
      "type": "code",
      "title": "normalizeAngle Method",
      "content": "Normalize angle to be within -π to π radians.",
      "code": "    private double normalizeAngle(double angle) {\n        while (angle > Math.PI) angle -= 2 * Math.PI;\n        while (angle < -Math.PI) angle += 2 * Math.PI;\n        return angle;\n    }"
    },
    {
      "type": "text",
      "title": "Public Access Methods",
      "content": "Public methods provide access to the robot's current position and orientation for other systems."
    },
    {
      "type": "code",
      "title": "Public Access Methods",
      "content": "Provide public methods to access robot pose information.",
      "code": "    // Public methods to access robot pose\n    public double getRobotX() { return robotX; }\n    public double getRobotY() { return robotY; }\n    public double getRobotHeading() { return robotHeading; }\n    public Point2D getRobotPosition() { return new Point2D(robotX, robotY); }\n    \n    // Helper class for 2D points\n    public static class Point2D {\n        public double x, y;\n        \n        public Point2D(double x, double y) {\n            this.x = x;\n            this.y = y;\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Path Planning with Visual Feedback",
      "content": "Path planning with visual feedback uses vision data to dynamically plan and adjust robot movement paths. This enables robots to navigate around obstacles, follow visual paths, and reach targets with high precision.<br><br><strong>Dynamic Path Planning:</strong> Adjusting paths based on real-time visual information<br><strong>Obstacle Detection:</strong> Identifying and avoiding obstacles using vision<br><strong>Visual Path Following:</strong> Following visual markers or lines<br><strong>Target Tracking:</strong> Continuously tracking and approaching visual targets<br><strong>Path Optimization:</strong> Finding optimal paths considering visual constraints<br><strong>Real-time Adjustment:</strong> Modifying paths during execution based on new visual data"
    },
    {
      "type": "code",
      "title": "Visual Path Planning System",
      "content": "Example of a visual path planning system with obstacle avoidance:",
      "code": "public class VisualPathPlanningSystem {\n    private VisionLocalizationSystem localization;\n    private List<Point2D> pathPoints = new ArrayList<>();\n    private List<Obstacle> detectedObstacles = new ArrayList<>();\n    \n    // Path planning parameters\n    private static final double PATH_RESOLUTION = 2.0; // inches\n    private static final double OBSTACLE_SAFETY_MARGIN = 6.0; // inches\n    private static final double MAX_PATH_LENGTH = 200.0; // inches\n    \n    public VisualPathPlanningSystem(VisionLocalizationSystem localization) {\n        this.localization = localization;\n    }\n    \n    public List<Point2D> planPathToTarget(Point2D target) {\n        Point2D currentPos = localization.getRobotPosition();\n        \n        // Clear previous path\n        pathPoints.clear();\n        \n        // Simple straight-line path planning with obstacle avoidance\n        List<Point2D> directPath = createDirectPath(currentPos, target);\n        \n        // Check for obstacles and modify path if necessary\n        List<Point2D> safePath = avoidObstacles(directPath);\n        \n        // Smooth the path for better robot following\n        List<Point2D> smoothedPath = smoothPath(safePath);\n        \n        pathPoints = smoothedPath;\n        return new ArrayList<>(pathPoints);\n    }\n    \n    private List<Point2D> createDirectPath(Point2D start, Point2D end) {\n        List<Point2D> path = new ArrayList<>();\n        \n        double distance = Math.sqrt(Math.pow(end.x - start.x, 2) + Math.pow(end.y - start.y, 2));\n        int numPoints = (int) Math.ceil(distance / PATH_RESOLUTION);\n        \n        for (int i = 0; i <= numPoints; i++) {\n            double t = (double) i / numPoints;\n            double x = start.x + t * (end.x - start.x);\n            double y = start.y + t * (end.y - start.y);\n            path.add(new Point2D(x, y));\n        }\n        \n        return path;\n    }\n    \n    private List<Point2D> avoidObstacles(List<Point2D> path) {\n        List<Point2D> safePath = new ArrayList<>();\n        \n        for (int i = 0; i < path.size(); i++) {\n            Point2D point = path.get(i);\n            \n            // Check if point is too close to any obstacle\n            boolean isSafe = true;\n            for (Obstacle obstacle : detectedObstacles) {\n                double distance = Math.sqrt(Math.pow(point.x - obstacle.center.x, 2) + \n                                          Math.pow(point.y - obstacle.center.y, 2));\n                if (distance < obstacle.radius + OBSTACLE_SAFETY_MARGIN) {\n                    isSafe = false;\n                    break;\n                }\n            }\n            \n            if (isSafe) {\n                safePath.add(point);\n            } else {\n                // Find alternative path around obstacles\n                Point2D alternativePoint = findAlternativePoint(point);\n                if (alternativePoint != null) {\n                    safePath.add(alternativePoint);\n                }\n            }\n        }\n        \n        return safePath;\n    }\n    \n    private Point2D findAlternativePoint(Point2D blockedPoint) {\n        // Simple obstacle avoidance - move perpendicular to obstacle\n        for (Obstacle obstacle : detectedObstacles) {\n            double distance = Math.sqrt(Math.pow(blockedPoint.x - obstacle.center.x, 2) + \n                                      Math.pow(blockedPoint.y - obstacle.center.y, 2));\n            \n            if (distance < obstacle.radius + OBSTACLE_SAFETY_MARGIN) {\n                // Calculate perpendicular direction\n                double dx = blockedPoint.x - obstacle.center.x;\n                double dy = blockedPoint.y - obstacle.center.y;\n                double length = Math.sqrt(dx * dx + dy * dy);\n                \n                if (length > 0) {\n                    // Normalize and scale to safety margin\n                    double scale = (obstacle.radius + OBSTACLE_SAFETY_MARGIN) / length;\n                    double newX = obstacle.center.x + dx * scale;\n                    double newY = obstacle.center.y + dy * scale;\n                    \n                    return new Point2D(newX, newY);\n                }\n            }\n        }\n        \n        return null;\n    }\n    \n    private List<Point2D> smoothPath(List<Point2D> path) {\n        if (path.size() < 3) {\n            return new ArrayList<>(path);\n        }\n        \n        List<Point2D> smoothed = new ArrayList<>();\n        smoothed.add(path.get(0)); // Keep start point\n        \n        // Simple smoothing - average adjacent points\n        for (int i = 1; i < path.size() - 1; i++) {\n            Point2D prev = path.get(i - 1);\n            Point2D curr = path.get(i);\n            Point2D next = path.get(i + 1);\n            \n            // Weighted average (current point has more weight)\n            double x = 0.25 * prev.x + 0.5 * curr.x + 0.25 * next.x;\n            double y = 0.25 * prev.y + 0.5 * curr.y + 0.25 * next.y;\n            \n            smoothed.add(new Point2D(x, y));\n        }\n        \n        smoothed.add(path.get(path.size() - 1)); // Keep end point\n        return smoothed;\n    }\n    \n    public void updateObstacles(List<Obstacle> obstacles) {\n        detectedObstacles = new ArrayList<>(obstacles);\n    }\n    \n    public List<Point2D> getCurrentPath() {\n        return new ArrayList<>(pathPoints);\n    }\n    \n    // Obstacle class\n    public static class Obstacle {\n        public Point2D center;\n        public double radius;\n        \n        public Obstacle(Point2D center, double radius) {\n            this.center = center;\n            this.radius = radius;\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Visual Servoing and Alignment",
      "content": "Visual servoing uses visual feedback to control robot movement for precise positioning and alignment. This technique is essential for tasks requiring high precision, such as scoring game elements or aligning with field features.<br><br><strong>Visual Servoing Types:</strong> Image-based, position-based, and hybrid approaches<br><strong>Alignment Control:</strong> Using visual feedback to achieve precise alignment<br><strong>Target Tracking:</strong> Continuously tracking and following visual targets<br><strong>Precision Positioning:</strong> Achieving sub-inch positioning accuracy<br><strong>Servoing Algorithms:</strong> Proportional, PID, and advanced control methods<br><strong>Error Correction:</strong> Continuously correcting position and orientation errors"
    },
    {
      "type": "code",
      "title": "Visual Servoing System",
      "content": "Example of a visual servoing system for precise alignment:",
      "code": "public class VisualServoingSystem {\n    private VisionLocalizationSystem localization;\n    private DcMotor leftDrive, rightDrive;\n    \n    // Servoing parameters\n    private static final double POSITION_TOLERANCE = 1.0; // inches\n    private static final double ANGLE_TOLERANCE = Math.toRadians(2.0); // radians\n    private static final double MAX_SPEED = 0.5;\n    private static final double MIN_SPEED = 0.1;\n    \n    // PID controllers for position and angle control\n    private PIDController xController = new PIDController(0.1, 0.0, 0.01);\n    private PIDController yController = new PIDController(0.1, 0.0, 0.01);\n    private PIDController angleController = new PIDController(0.05, 0.0, 0.005);\n    \n    public VisualServoingSystem(VisionLocalizationSystem localization, \n                               DcMotor leftDrive, DcMotor rightDrive) {\n        this.localization = localization;\n        this.leftDrive = leftDrive;\n        this.rightDrive = rightDrive;\n        \n        // Configure PID controllers\n        xController.setSetpoint(0.0);\n        yController.setSetpoint(0.0);\n        angleController.setSetpoint(0.0);\n    }\n    \n    public boolean servoToTarget(Point2D target, double targetAngle) {\n        Point2D currentPos = localization.getRobotPosition();\n        double currentAngle = localization.getRobotHeading();\n        \n        // Calculate position errors\n        double xError = target.x - currentPos.x;\n        double yError = target.y - currentPos.y;\n        double angleError = normalizeAngle(targetAngle - currentAngle);\n        \n        // Check if we're close enough to target\n        boolean positionReached = Math.abs(xError) < POSITION_TOLERANCE && \n                                 Math.abs(yError) < POSITION_TOLERANCE;\n        boolean angleReached = Math.abs(angleError) < ANGLE_TOLERANCE;\n        \n        if (positionReached && angleReached) {\n            // Target reached - stop robot\n            stopRobot();\n            return true;\n        }\n        \n        // Update PID controllers\n        xController.setSetpoint(target.x);\n        yController.setSetpoint(target.y);\n        angleController.setSetpoint(targetAngle);\n        \n        // Calculate control outputs\n        double xOutput = xController.calculate(currentPos.x);\n        double yOutput = yController.calculate(currentPos.y);\n        double angleOutput = angleController.calculate(currentAngle);\n        \n        // Combine position and angle control\n        double forwardPower = yOutput; // Forward/backward\n        double strafePower = xOutput;  // Left/right\n        double turnPower = angleOutput; // Rotation\n        \n        // Apply motor commands (assuming mecanum drive)\n        applyMecanumDrive(forwardPower, strafePower, turnPower);\n        \n        return false; // Not yet reached target\n    }\n    \n    public boolean servoToVisualTarget(AprilTagDetection targetTag, double targetDistance) {\n        if (targetTag == null) {\n            return false;\n        }\n        \n        AprilTagPoseFtc tagPose = targetTag.ftcPose;\n        \n        // Calculate desired position relative to tag\n        double desiredX = 0.0; // Centered on tag\n        double desiredY = targetDistance; // Desired distance from tag\n        double desiredAngle = 0.0; // Facing tag directly\n        \n        // Calculate current errors\n        double xError = desiredX - tagPose.x;\n        double yError = desiredY - tagPose.z; // Z is distance\n        double angleError = normalizeAngle(desiredAngle - tagPose.yaw);\n        \n        // Check if target reached\n        boolean positionReached = Math.abs(xError) < POSITION_TOLERANCE && \n                                 Math.abs(yError) < POSITION_TOLERANCE;\n        boolean angleReached = Math.abs(angleError) < ANGLE_TOLERANCE;\n        \n        if (positionReached && angleReached) {\n            stopRobot();\n            return true;\n        }\n        \n        // Simple proportional control for visual servoing\n        double xPower = 0.02 * xError; // Lateral movement\n        double yPower = 0.02 * yError; // Forward/backward\n        double turnPower = 0.02 * angleError; // Rotation\n        \n        // Limit speeds\n        xPower = Math.max(-MAX_SPEED, Math.min(MAX_SPEED, xPower));\n        yPower = Math.max(-MAX_SPEED, Math.min(MAX_SPEED, yPower));\n        turnPower = Math.max(-MAX_SPEED, Math.min(MAX_SPEED, turnPower));\n        \n        // Apply minimum speed threshold\n        if (Math.abs(xPower) < MIN_SPEED) xPower = 0;\n        if (Math.abs(yPower) < MIN_SPEED) yPower = 0;\n        if (Math.abs(turnPower) < MIN_SPEED) turnPower = 0;\n        \n        applyMecanumDrive(yPower, xPower, turnPower);\n        \n        return false;\n    }\n    \n    private void applyMecanumDrive(double forward, double strafe, double turn) {\n        // Mecanum drive equations\n        double leftFrontPower = forward + strafe + turn;\n        double rightFrontPower = forward - strafe - turn;\n        double leftRearPower = forward - strafe + turn;\n        double rightRearPower = forward + strafe - turn;\n        \n        // Normalize powers to prevent exceeding maximum\n        double maxPower = Math.max(1.0, Math.max(Math.abs(leftFrontPower), \n            Math.max(Math.abs(rightFrontPower), \n            Math.max(Math.abs(leftRearPower), Math.abs(rightRearPower)))));\n        \n        if (maxPower > 1.0) {\n            leftFrontPower /= maxPower;\n            rightFrontPower /= maxPower;\n            leftRearPower /= maxPower;\n            rightRearPower /= maxPower;\n        }\n        \n        // Apply to motors (assuming 4-wheel mecanum)\n        // You would need to add the other motors to your hardware map\n        leftDrive.setPower(leftFrontPower);\n        rightDrive.setPower(rightFrontPower);\n        // leftRear.setPower(leftRearPower);\n        // rightRear.setPower(rightRearPower);\n    }\n    \n    private void stopRobot() {\n        leftDrive.setPower(0);\n        rightDrive.setPower(0);\n        // leftRear.setPower(0);\n        // rightRear.setPower(0);\n    }\n    \n    private double normalizeAngle(double angle) {\n        while (angle > Math.PI) angle -= 2 * Math.PI;\n        while (angle < -Math.PI) angle += 2 * Math.PI;\n        return angle;\n    }\n    \n    // Simple PID controller class\n    public static class PIDController {\n        private double kp, ki, kd;\n        private double setpoint;\n        private double previousError = 0;\n        private double integral = 0;\n        \n        public PIDController(double kp, double ki, double kd) {\n            this.kp = kp;\n            this.ki = ki;\n            this.kd = kd;\n        }\n        \n        public void setSetpoint(double setpoint) {\n            this.setpoint = setpoint;\n        }\n        \n        public double calculate(double measurement) {\n            double error = setpoint - measurement;\n            integral += error;\n            double derivative = error - previousError;\n            \n            double output = kp * error + ki * integral + kd * derivative;\n            \n            previousError = error;\n            return output;\n        }\n    }\n}"
    },
    {
      "type": "text",
      "title": "Multi-Sensor Fusion",
      "content": "Multi-sensor fusion combines vision data with other sensors to create more robust and accurate navigation systems. This approach provides redundancy and improves reliability in challenging conditions.<br><br><strong>Sensor Integration:</strong> Combining vision with encoders, IMU, and other sensors<br><strong>Data Fusion Algorithms:</strong> Kalman filters, particle filters, and other fusion methods<br><strong>Redundancy and Reliability:</strong> Using multiple sensors for fault tolerance<br><strong>Sensor Calibration:</strong> Aligning different sensor coordinate systems<br><strong>Fusion Strategies:</strong> Complementary, competitive, and cooperative fusion approaches<br><strong>Performance Optimization:</strong> Balancing accuracy with computational complexity"
    },
    {
      "type": "emphasis-box",
      "title": "Vision-Based Navigation Best Practices",
      "content": "To create effective vision-based navigation systems:<br><br><strong>System Design:</strong><br>• Start with simple navigation tasks and add complexity<br>• Use multiple sensors for redundancy and reliability<br>• Implement proper error handling and fallback behaviors<br>• Test thoroughly in various lighting and field conditions<br><br><strong>Performance:</strong><br>• Optimize vision processing for real-time operation<br>• Use appropriate control algorithms for your robot's dynamics<br>• Monitor and log navigation performance for debugging<br>• Implement safety limits and emergency stops<br><br><strong>Integration:</strong><br>• Design clear interfaces between vision and control systems<br>• Provide comprehensive telemetry for debugging and tuning<br>• Plan for graceful degradation when vision fails<br>• Test integration with other robot subsystems"
    },
    {
      "type": "exercise-box",
      "title": "Complete Vision Navigation System",
      "description": "Create a complete vision-based navigation system",
      "tasks": [
        "Implement vision-based localization using AprilTags",
        "Create a path planning system with obstacle avoidance",
        "Develop visual servoing for precise positioning",
        "Integrate multiple sensors for robust navigation",
        "Add error handling and fallback behaviors",
        "Test the system in various field conditions"
      ],
      "content": "Develop a complete vision-based navigation system that can localize the robot, plan paths to targets, avoid obstacles, and achieve precise positioning. The system should be robust, efficient, and suitable for autonomous competition use."
    },
    {
      "type": "link-grid",
      "title": "Additional Resources",
      "links": [
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/programming_resources/vision/vision-navigation.html\" target=\"_blank\">FTC Vision Navigation</a>",
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/apriltag/vision_portal/apriltag_intro/apriltag-intro.html\" target=\"_blank\">FTC AprilTag Documentation</a>",
        "<a href=\"https://gm0.org/en/latest/docs/software/tutorials/vision-navigation.html\" target=\"_blank\">gm0: Vision Navigation Tutorial</a>",
        "<a href=\"https://opencv.org/\" target=\"_blank\">OpenCV Documentation</a>"
      ]
    }
  ]
}
