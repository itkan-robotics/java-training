{
  "title": "Vision Introduction",
  "sections": [
    {
      "type": "text",
      "title": "What is Computer Vision?",
      "content": "Computer vision is the field of artificial intelligence that enables computers to interpret and understand visual information from the world around them. In FTC robotics, computer vision allows your robot to 'see' and make intelligent decisions based on what it observes through its camera.<br><br>Think of computer vision like human vision: just as your eyes capture light and your brain processes that information to understand what you're seeing, a robot's camera captures images and computer vision algorithms process that data to extract meaningful information."
    },
    {
      "type": "rules-box",
      "title": "Why Computer Vision Matters in FTC",
      "items": [
        "Autonomous navigation and positioning using visual landmarks",
        "Object detection and classification for game elements",
        "Precise alignment and targeting for scoring mechanisms",
        "Obstacle detection and avoidance",
        "Team element identification and tracking",
        "Field state analysis and strategic decision making"
      ]
    },
    {
      "type": "text",
      "title": "FTC Vision System Overview",
      "content": "FTC provides a comprehensive vision system built around the Control Hub's integrated camera and the Expansion Hub's processing capabilities. The system is designed to be both powerful and accessible, offering multiple approaches from simple color detection to advanced machine learning.<br><br>The FTC vision ecosystem includes:<br>• <strong>Vision Portal:</strong> User-friendly interface for vision processing<br>• <strong>OpenCV Integration:</strong> Advanced computer vision library<br>• <strong>TensorFlow Lite:</strong> Machine learning capabilities<br>• <strong>AprilTag Support:</strong> Fiducial marker detection"
    },
    {
      "type": "text",
      "title": "Vision Pipeline Architecture",
      "content": "A vision pipeline is a sequence of steps that transforms raw camera images into useful information for the robot. Understanding each stage helps you design effective and reliable vision systems.<br><br>The standard FTC vision pipeline consists of four main stages:<br><br><strong>1. Image Capture:</strong> The camera takes a picture of the environment, providing the raw data for processing.<br><strong>2. Preprocessing:</strong> The captured image is prepared for analysis. This can include adjusting brightness, reducing noise, or converting the image to a different color format to make it easier to work with.<br><strong>3. Feature Detection:</strong> The system looks for important elements in the image, such as specific colors, shapes, or patterns that are relevant to the robot's tasks.<br><strong>4. Analysis & Output:</strong> The detected features are interpreted to make decisions or provide information to other parts of the robot, such as navigation or scoring mechanisms."
    },
    {
      "type": "text",
      "title": "Understanding Each Stage",
      "content": "Let's break down what happens at each stage of a typical vision pipeline in FTC:<br><br><strong>Image Capture:</strong> The process begins when the robot's camera captures a frame. This is the raw visual input from the robot's environment.<br><br><strong>Preprocessing:</strong> Before analyzing the image, it is often enhanced or converted to a format that makes it easier to detect features. This might involve changing the color space, filtering out noise, or adjusting contrast.<br><br><strong>Feature Detection:</strong> In this stage, the system searches for specific characteristics in the image, such as colored regions, lines, or shapes. The goal is to identify objects or markers that are important for the robot's operation.<br><br><strong>Analysis & Output:</strong> Finally, the information gathered from feature detection is analyzed to determine what actions the robot should take. The results are then sent to other subsystems or displayed for the driver."
    },
    {
      "type": "emphasis-box",
      "title": "Performance Considerations",
      "content": "Vision processing is computationally intensive and can significantly impact your robot's performance. Key considerations include:<br><br><strong>Frame Rate:</strong> Higher frame rates provide more responsive vision but require more processing power<br><strong>Resolution:</strong> Higher resolution provides more detail but increases processing time<br><strong>Processing Complexity:</strong> More complex algorithms provide better results but take longer to execute<br><strong>Memory Usage:</strong> Vision processing can consume significant memory resources<br><br>For FTC applications, aim for 10-30 FPS with reasonable resolution (640x480 or 1280x720) to balance performance and responsiveness."
    },
    {
      "type": "text",
      "title": "Performance Monitoring Pipeline Structure",
      "content": "Monitoring the performance of your vision pipeline is important for ensuring your robot responds quickly. Let's break down how to do this. For more on performance, see <a href=\"https://gm0.org/en/latest/docs/software/tutorials/vision.html#performance-considerations\" target=\"_blank\">gm0: Vision Performance</a>."
    },
    {
      "type": "text",
      "title": "Setting Up Performance Tracking",
      "content": "To monitor your vision pipeline's performance, you need to track several key metrics. Start by creating variables to store timing information, including when each frame processing begins, how many frames have been processed, and the calculated frames per second (FPS). These variables will help you measure the efficiency of your vision processing."
    },
    {
      "type": "text",
      "title": "Measuring Processing Time",
      "content": "The core of performance monitoring happens in your pipeline's main processing method. Before starting any vision processing, record the current time using a high-precision timer. After completing all vision processing steps, calculate the total time taken by subtracting the start time from the current time. This gives you the processing time for each frame."
    },
    {
      "type": "text",
      "title": "Calculating Frame Rate",
      "content": "Frame rate calculation involves counting the number of frames processed and calculating the average FPS. Rather than updating the FPS calculation every frame (which can be computationally expensive), update it periodically, such as every 30 frames. This provides a good balance between accuracy and performance."
    },
    {
      "type": "text",
      "title": "Displaying Performance Data",
      "content": "Once you have calculated the performance metrics, display them on the Driver Station telemetry. Show both the processing time in milliseconds and the current FPS. This allows you to monitor performance in real-time and identify when your vision pipeline is running too slowly. Learn more about telemetry: <a href=\"https://ftc-docs.firstinspires.org/en/latest/programming_resources/driver_station/telemetry.html\" target=\"_blank\">FTC Telemetry Docs</a>."
    },
    {
      "type": "exercise-box",
      "title": "Vision System Planning Exercise",
      "description": "Plan a vision system for the 2025 FTC game, Into the Deep",
      "tasks": [
        "Identify what visual information your robot needs to detect",
        "Determine the optimal camera placement and orientation",
        "Choose appropriate vision processing techniques (color detection, AprilTags, ML, etc.)",
        "Estimate the required frame rate and resolution",
        "Plan how vision data will integrate with other robot subsystems"
      ],
      "content": "Scenario: Your robot needs to detect game elements of different colors and navigate to specific positions marked by AprilTags. The robot operates in varying lighting conditions and must respond quickly to changing field conditions.",
      "answers": [
        {
          "task": "Identify what visual information your robot needs to detect",
          "content": "// For the Into the Deep game scenario, your robot needs to detect:\n\n// Game Elements:\n// • Colored game pieces (different colors for scoring)\n// • Team alliance elements (blue/red team identification)\n// • Scoring targets and zones\n\n// Navigation Landmarks:\n// • AprilTags for precise positioning\n// • Field boundaries and obstacles\n// • Alliance station locations\n\n// Dynamic Elements:\n// • Opponent robots for collision avoidance\n// • Moving game elements\n// • Changing field conditions"
        },
        {
          "task": "Determine the optimal camera placement and orientation",
          "content": "// Camera Placement Strategy:\n\n// Primary Camera (Front-facing):\n// • Mounted on the front of the robot at 6-12 inches height\n// • Angled slightly downward (15-20 degrees) for optimal field coverage\n// • Positioned to capture both ground-level game elements and AprilTags\n\n// Secondary Camera (Optional - Side-facing):\n// • Mounted on the side for lateral obstacle detection\n// • Helps with navigation and collision avoidance\n\n// Considerations:\n// • Avoid direct lighting interference\n// • Ensure stable mounting to prevent vibration\n// • Position away from moving mechanisms\n// • Consider field of view requirements (60-90 degrees recommended)"
        },
        {
          "task": "Choose appropriate vision processing techniques",
          "content": "// Recommended Vision Processing Stack:\n\n// Color Detection:\n// • Use HSV color space for better lighting robustness\n// • Implement color thresholding for game element identification\n// • Create color masks for different team alliances\n\n// AprilTag Detection:\n// • Use FTC's built-in AprilTag library\n// • Configure for 36h11 tag family\n// • Implement pose estimation for precise positioning\n\n// Object Detection:\n// • Use TensorFlow Lite for complex object recognition\n// • Train custom models for specific game elements\n// • Implement confidence thresholds for reliability\n\n// Edge Detection:\n// • Use OpenCV for field boundary detection\n// • Implement contour detection for obstacle avoidance"
        },
        {
          "task": "Estimate the required frame rate and resolution",
          "content": "// Performance Requirements:\n\n// Frame Rate:\n// • Target: 15-20 FPS for responsive navigation\n// • Minimum: 10 FPS for basic functionality\n// • Maximum: 30 FPS (hardware dependent)\n\n// Resolution:\n// • Primary: 640x480 (good balance of detail and performance)\n// • Alternative: 1280x720 for higher precision (if hardware supports)\n// • Consider downscaling for processing efficiency\n\n// Processing Pipeline:\n// • Image capture: 640x480 @ 20 FPS\n// • Preprocessing: Color space conversion, noise reduction\n// • Feature detection: Color thresholding, AprilTag detection\n// • Analysis: Pose estimation, decision making\n// • Total processing time target: <50ms per frame"
        },
        {
          "task": "Plan how vision data will integrate with other robot subsystems",
          "content": "// System Integration Architecture:\n\n// Navigation Subsystem:\n// • AprilTag pose data → Path planning algorithms\n// • Field boundary detection → Obstacle avoidance\n// • Real-time position updates for autonomous navigation\n\n// Scoring Subsystem:\n// • Game element detection → Scoring mechanism control\n// • Color identification → Team alliance verification\n// • Target alignment → Precision positioning\n\n// Driver Feedback:\n// • Telemetry display of detected elements\n// • Visual indicators for driver assistance\n// • Performance metrics monitoring\n\n// Safety Systems:\n// • Collision detection → Emergency stop protocols\n// • Opponent robot tracking → Evasive maneuvers\n// • Field boundary monitoring → Stay-in-bounds enforcement"
        },
        {
          "task": "Complete Vision System Implementation Plan",
          "content": "// Implementation Timeline:\n\n// Phase 1: Basic Setup (Week 1-2)\n// • Camera hardware installation and configuration\n// • Basic color detection implementation\n// • Simple AprilTag detection\n\n// Phase 2: Advanced Features (Week 3-4)\n// • Pose estimation and navigation integration\n// • Object detection with TensorFlow Lite\n// • Performance optimization\n\n// Phase 3: Integration (Week 5-6)\n// • Subsystem integration and testing\n// • Driver interface development\n// • Competition-ready refinement\n\n// Key Success Metrics:\n// • AprilTag detection accuracy: >95%\n// • Color detection reliability: >90%\n// • Processing latency: <50ms\n// • System uptime: >99% during matches"
        }
      ]
    },
    {
      "type": "link-grid",
      "title": "Additional Resources",
      "links": [
        "<a href=\"https://gm0.org/en/latest/docs/software/tutorials/vision.html\" target=\"_blank\">gm0: Vision Tutorial</a>",
        "<a href=\"https://ftc-docs.firstinspires.org/en/latest/apriltag/vision_portal/visionportal_overview/visionportal-overview.html\" target=\"_blank\">FTC Vision Overview</a>"
      ]
    }
  ]
}
