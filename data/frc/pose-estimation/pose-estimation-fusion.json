{
  "title": "Pose Estimation Sensor Fusion",
  "sections": [
    {
      "type": "text",
      "title": "Introduction to Sensor Fusion",
      "content": "Sensor fusion combines multiple pose estimation sources (odometry and vision) to achieve better accuracy and reliability than any single source alone. By combining continuous odometry updates with periodic absolute vision corrections, sensor fusion provides the best of both worlds: fast, continuous tracking with periodic drift correction.<br><br>Fusion algorithms intelligently weight different sensor sources based on their reliability, update rates, and confidence. When vision provides a pose estimate, fusion incorporates it to correct odometry drift. When vision is unavailable, fusion relies on odometry for continuous tracking. This creates a robust pose estimation system that works in all conditions.<br><br>Learn more: <a href='https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimator.html' target='_blank'>WPILib: Pose Estimator</a>"
    },
    {
      "type": "text",
      "title": "Fusion Principles",
      "content": "Understanding how sensor fusion works:<br><br><strong>Combining Odometry and Vision:</strong> Odometry provides continuous, fast updates (50 Hz) but accumulates error over time. Vision provides absolute positioning (no drift) but updates less frequently (10-30 Hz) and requires visibility. Fusion combines these by using odometry for continuous tracking and vision for periodic corrections.<br><br><strong>Weighting Sources:</strong> Fusion algorithms weight different sources based on their reliability and confidence. High-confidence vision measurements have more weight than low-confidence ones. Odometry is weighted based on time since last vision update - the longer since vision, the more odometry may have drifted.<br><br><strong>Handling Conflicts:</strong> When sensors disagree, fusion algorithms determine which source is more reliable. This might be based on confidence scores, recent history, or statistical analysis. The goal is to produce the most accurate pose estimate given all available information."
    },
    {
      "type": "text",
      "title": "Odometry + Vision Fusion",
      "content": "The most common fusion approach combines odometry and vision:<br><br><strong>How to Combine:</strong> Odometry runs continuously, providing pose updates every robot loop. Vision provides pose estimates when tags or features are visible. Fusion incorporates vision updates to correct odometry drift while maintaining continuous tracking.<br><br><strong>Update Strategy:</strong> When a vision update arrives, fusion compares it to the current odometry pose. If they agree (within tolerance), fusion smoothly incorporates the vision measurement. If they disagree significantly, fusion may weight the vision measurement more heavily (assuming vision is more accurate) or may reject it if it seems unreliable.<br><br><strong>Continuous Tracking:</strong> Between vision updates, fusion relies on odometry. This provides continuous pose estimation even when vision is unavailable. When vision becomes available again, it corrects any drift that accumulated."
    },
    {
      "type": "text",
      "title": "WPILib Pose Estimator",
      "content": "WPILib provides the <code>PoseEstimator</code> class for sensor fusion:<br><br><strong>PoseEstimator Class:</strong> Handles fusion of odometry and vision measurements. Uses Kalman filtering concepts for optimal fusion. Automatically weights measurements based on confidence and uncertainty. Provides current best pose estimate.<br><br><strong>Setup and Usage:</strong> Initialize with starting pose and kinematics. Add odometry measurements continuously (every robot loop). Add vision measurements when available (asynchronously). Query for current pose estimate anytime.<br><br><strong>Key Methods:</strong> <code>addVisionMeasurement()</code> - adds vision pose estimate with timestamp. <code>update()</code> - updates with odometry measurement. <code>getEstimatedPosition()</code> - returns current fused pose estimate. The class handles all the complex fusion math internally."
    },
    {
      "type": "code",
      "title": "PoseEstimator Setup",
      "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.geometry.Translation2d;\nimport edu.wpi.first.math.kinematics.SwerveDriveKinematics;\nimport edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport edu.wpi.first.math.kinematics.SwerveModulePosition;\nimport edu.wpi.first.math.VecBuilder;\nimport edu.wpi.first.wpilibj.Timer;\n\npublic class FusedPoseEstimation {\n    // Kinematics for odometry\n    private final SwerveDriveKinematics m_kinematics;\n    \n    // Pose estimator for sensor fusion\n    private SwerveDrivePoseEstimator m_poseEstimator;\n    \n    // Module positions (for odometry)\n    private SwerveModulePosition[] m_modulePositions;\n    \n    // Gyro for heading\n    private final ADXRS450_Gyro m_gyro;\n    \n    // Swerve modules\n    private SwerveModule[] m_modules;\n    \n    // Vision system\n    private final VisionPoseEstimation m_vision;\n    \n    // Robot dimensions: module positions relative to robot center (in meters)\n    private static final double kTrackWidth = 0.5;  // Distance between left/right modules\n    private static final double kWheelBase = 0.5;    // Distance between front/rear modules\n    \n    public FusedPoseEstimation() {\n        // Define module positions relative to robot center\n        Translation2d frontLeft = new Translation2d(kWheelBase / 2, kTrackWidth / 2);\n        Translation2d frontRight = new Translation2d(kWheelBase / 2, -kTrackWidth / 2);\n        Translation2d rearLeft = new Translation2d(-kWheelBase / 2, kTrackWidth / 2);\n        Translation2d rearRight = new Translation2d(-kWheelBase / 2, -kTrackWidth / 2);\n        \n        m_kinematics = new SwerveDriveKinematics(\n            frontLeft, frontRight, rearLeft, rearRight\n        );\n        \n        m_gyro = new ADXRS450_Gyro();\n        m_gyro.calibrate();\n        \n        m_modulePositions = new SwerveModulePosition[4];\n        for (int i = 0; i < 4; i++) {\n            m_modulePositions[i] = new SwerveModulePosition();\n        }\n        \n        // (In real code, these would come from your SwerveModule objects)\n        m_modulePositions[0] = m_modules[0].getPosition();  // Front left\n        m_modulePositions[1] = m_modules[1].getPosition();  // Front right\n        m_modulePositions[2] = m_modules[2].getPosition();  // Rear left\n        m_modulePositions[3] = m_modules[3].getPosition();  // Rear right\n        \n        // Standard deviations: how much we trust each measurement\n        // Lower values = more trust\n        // Note: VecBuilder requires import: edu.wpi.first.math.VecBuilder\n        var stateStdDevs = VecBuilder.fill(0.1, 0.1, 0.1);  // X, Y, heading uncertainty\n        var visionStdDevs = VecBuilder.fill(0.5, 0.5, 0.5);  // Vision measurement uncertainty\n        \n        m_poseEstimator = new SwerveDrivePoseEstimator(\n            m_kinematics,\n            m_gyro.getRotation2d(),\n            m_modulePositions,\n            new Pose2d(),\n            stateStdDevs,\n            visionStdDevs\n        );\n        \n        m_vision = new VisionPoseEstimation();\n    }\n}"
    },
    {
      "type": "code",
      "title": "Adding Odometry Measurements",
      "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.kinematics.SwerveModulePosition;\n\npublic class FusedPoseEstimation {\n    private SwerveDrivePoseEstimator m_poseEstimator;\n    private SwerveModulePosition[] m_modulePositions;\n    private final ADXRS450_Gyro m_gyro;\n    private SwerveModule[] m_modules;\n    \n    // ... initialization code ...\n    \n    /**\n     * Update pose estimator with odometry - call this every robot loop\n     */\n    public void updateOdometry() {\n        m_modulePositions[0] = m_modules[0].getPosition();  // Front left\n        m_modulePositions[1] = m_modules[1].getPosition();  // Front right\n        m_modulePositions[2] = m_modules[2].getPosition();  // Rear left\n        m_modulePositions[3] = m_modules[3].getPosition();  // Rear right\n        \n        Rotation2d heading = m_gyro.getRotation2d();\n        \n        m_poseEstimator.update(heading, m_modulePositions);\n    }\n    \n    /**\n     * Get current fused pose estimate\n     */\n    public Pose2d getPose() {\n        return m_poseEstimator.getEstimatedPosition();\n    }\n}"
    },
    {
      "type": "code",
      "title": "Adding Vision Measurements",
      "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport edu.wpi.first.wpilibj.Timer;\nimport java.util.Optional;\n\npublic class FusedPoseEstimation {\n    private SwerveDrivePoseEstimator m_poseEstimator;\n    private final VisionPoseEstimation m_vision;\n    \n    // ... initialization and odometry update code ...\n    \n    /**\n     * Add vision measurement when available\n     * Call this when vision provides a pose estimate\n     */\n    public void updateVision() {\n        Optional<Pose2d> visionPose = m_vision.getVisionPose();\n        \n        if (visionPose.isPresent()) {\n            double timestamp = Timer.getFPGATimestamp();\n            \n            // Add vision measurement to pose estimator\n            // The estimator will fuse this with odometry\n            m_poseEstimator.addVisionMeasurement(\n                visionPose.get(),\n                timestamp\n            );\n        }\n    }\n    \n    /**\n     * Complete update: odometry + vision\n     * Call this in periodic()\n     */\n    public void periodic() {\n        updateOdometry();\n        \n        updateVision();\n        \n        // Pose estimator now has best fused estimate\n        Pose2d currentPose = getPose();\n    }\n}"
    },
    {
      "type": "code",
      "title": "Reading Fused Pose Estimate",
      "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\n\npublic class FusedPoseEstimation {\n    private SwerveDrivePoseEstimator m_poseEstimator;\n    \n    // ... setup and update code ...\n    \n    public void periodic() {\n        updateOdometry();\n        updateVision();\n        \n        Pose2d fusedPose = getPose();\n        \n        SmartDashboard.putNumber(\"Fused X\", fusedPose.getX());\n        SmartDashboard.putNumber(\"Fused Y\", fusedPose.getY());\n        SmartDashboard.putNumber(\"Fused Heading\", \n            fusedPose.getRotation().getDegrees());\n        \n        // This pose combines odometry and vision for best accuracy\n        Pose2d targetPose = new Pose2d(3.0, 2.0, Rotation2d.fromDegrees(90.0));\n        double distanceToTarget = fusedPose.getTranslation()\n            .getDistance(targetPose.getTranslation());\n        \n        SmartDashboard.putNumber(\"Distance to Target (Fused)\", distanceToTarget);\n    }\n}"
    },
    {
      "type": "text",
      "title": "Fusion Algorithms",
      "content": "Understanding fusion algorithm concepts:<br><br><strong>Kalman Filtering Concepts:</strong> Pose estimators use Kalman filtering principles. Kalman filters maintain an estimate of the system state (pose) and its uncertainty. When new measurements arrive, the filter updates the estimate by weighting the measurement against current uncertainty. Measurements with lower uncertainty (higher confidence) have more influence.<br><br><strong>Weighted Averaging:</strong> Fusion combines measurements using weighted averages. Each measurement is weighted by its confidence or inverse uncertainty. High-confidence measurements have more weight. The result is an optimal combination that minimizes overall uncertainty.<br><br><strong>Update Strategies:</strong> Different strategies for incorporating measurements: immediate updates (use vision as soon as available), delayed updates (buffer and process together), or adaptive weighting (adjust weights based on measurement quality). WPILib's PoseEstimator uses sophisticated algorithms internally."
    },
    {
      "type": "text",
      "title": "Handling Conflicts",
      "content": "When sensors disagree, fusion must determine the best approach:<br><br><strong>What to Do When Sensors Disagree:</strong> Small disagreements are normal - fusion algorithms handle these automatically. Large disagreements may indicate sensor problems. Fusion may reject obviously bad measurements or weight them less heavily.<br><br><strong>Confidence Weighting:</strong> Measurements with higher confidence scores have more influence. Vision systems typically provide confidence scores. Low-confidence vision measurements may be rejected or weighted less. Odometry confidence decreases over time since last vision update.<br><br><strong>Validation:</strong> Fusion algorithms may validate measurements before incorporating them. Measurements that are too different from current estimate may be rejected. This prevents bad measurements from corrupting the pose estimate."
    },
    {
      "type": "text",
      "title": "Fusion Update Rates",
      "content": "Understanding update rate considerations:<br><br><strong>How Often to Update:</strong> Odometry should update every robot loop (50 Hz) for continuous tracking. Vision updates are asynchronous - add them when available (typically 10-30 Hz). Fusion handles different update rates automatically.<br><br><strong>Balancing Sources:</strong> Fusion balances odometry and vision based on their update rates and reliability. When vision is updating frequently, it has more influence. When vision is unavailable, fusion relies on odometry. The balance adapts automatically based on measurement availability and confidence."
    },
    {
      "type": "link-grid",
      "title": "Related Topics and Documentation",
      "links": [
        {
          "label": "WPILib Pose Estimator",
          "url": "https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimator.html"
        },
        {
          "label": "SwerveDrivePoseEstimator",
          "url": "https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimator.html"
        },
        {
          "label": "WPILib Odometry",
          "url": "https://docs.wpilib.org/en/stable/docs/software/kinematics-and-odometry/index.html#kinematics-and-odometry"
        }
      ]
    }
  ]
}