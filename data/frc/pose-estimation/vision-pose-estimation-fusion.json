{
    "title": "Fused Pose Estimation",
    "sections": [
        {
            "type": "text",
            "title": "Introduction to Sensor Fusion",
            "content": "Sensor fusion combines odometry and vision-based pose estimation to create a robust, accurate localization system. This lesson covers how to use WPILib's PoseEstimator to fuse odometry measurements with vision corrections.<br><br>The code examples below show the general structure and flow for integrating vision with pose estimation. Your team's drivetrain template will have the complete, hardware-specific implementation with all the details needed for your specific robot configuration.<br><br>Learn more: <a href='https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimators.html#pose-estimators' target='_blank'>WPILib: Pose Estimator Documentation</a>"
        },
        {
            "type": "text",
            "title": "The Power of Sensor Fusion",
            "content": "Vision provides excellent absolute positioning, but it has limitations. Cameras can't always see tags—other robots might block them, lighting might be poor, or the camera might be looking in the wrong direction. Meanwhile, odometry provides continuous updates at 50 Hz, but it drifts over time.<br><br>Sensor fusion combines these complementary strengths. Think of it as having two navigation systems: odometry is your speedometer and compass (always working, but accumulating error), while vision is your GPS (occasionally available, but always accurate when it works). By intelligently combining both, you get continuous, accurate pose estimation that works in all conditions."
        },
        {
            "type": "text",
            "title": "How Fusion Works",
            "content": "WPILib's PoseEstimator uses Kalman filtering principles to optimally combine measurements. Here's the concept:<br><br><strong>Continuous Odometry Updates</strong> - Every robot loop (50 times per second), odometry provides a pose update based on wheel movement and gyro readings. The estimator uses these for continuous tracking.<br><br><strong>Periodic Vision Corrections</strong> - When vision detects a tag (10-30 times per second), it provides an absolute pose measurement. The estimator compares this to the current odometry-based estimate and smoothly incorporates the correction.<br><br><strong>Intelligent Weighting</strong> - The estimator automatically weights measurements based on their uncertainty. High-confidence vision measurements have more influence than low-confidence ones. Odometry confidence decreases over time since the last vision update, naturally giving more weight to fresh vision corrections."
        },
        {
            "type": "text",
            "title": "Setting Up the Pose Estimator",
            "content": "The PoseEstimator needs kinematics, sensor readings, and uncertainty parameters. Let's set it up step by step. The kinematics define your robot's geometry, the sensor readings provide initial state, and the uncertainty parameters control how much weight each measurement type receives."
        },
        {
            "type": "code",
            "title": "Defining Kinematics",
            "description": "First, we define the robot's kinematics—the geometry of how modules are positioned relative to the robot center. For a swerve drive, you need to specify the position of each swerve module relative to the robot's center of rotation. This is typically done using Translation2d objects that represent the X and Y offsets of each module.",
            "content": "import edu.wpi.first.math.geometry.Translation2d;\nimport edu.wpi.first.math.kinematics.SwerveDriveKinematics;\n\n// Robot dimensions (measured from your robot, in meters)\nprivate static final double kTrackWidth = 0.5;\nprivate static final double kWheelBase = 0.5;\n\n// Calculate module positions from robot center\nTranslation2d frontLeft = new Translation2d(kWheelBase / 2.0, kTrackWidth / 2.0);\nTranslation2d frontRight = new Translation2d(kWheelBase / 2.0, -kTrackWidth / 2.0);\nTranslation2d rearLeft = new Translation2d(-kWheelBase / 2.0, kTrackWidth / 2.0);\nTranslation2d rearRight = new Translation2d(-kWheelBase / 2.0, -kTrackWidth / 2.0);\n\n// Create kinematics object\nSwerveDriveKinematics m_kinematics = new SwerveDriveKinematics(\n    frontLeft, frontRight, rearLeft, rearRight\n);"
        },
        {
            "type": "code",
            "title": "Configuring Standard Deviations",
            "description": "Standard deviations control how much we trust each measurement. Lower values mean more trust. The state standard deviations represent how much uncertainty exists in your odometry measurements, while vision standard deviations represent the uncertainty in vision measurements. These values directly affect how the pose estimator weights different measurements.",
            "content": "import edu.wpi.first.math.VecBuilder;\n\n// State uncertainty: how much odometry might drift\n// Tune based on your encoder and gyro accuracy\nvar stateStdDevs = VecBuilder.fill(0.1, 0.1, 0.1);\n\n// Vision uncertainty: how accurate vision measurements are\n// Tune based on camera quality and tag detection accuracy\nvar visionStdDevs = VecBuilder.fill(0.5, 0.5, 0.5);"
        },
        {
            "type": "code",
            "title": "Creating the Pose Estimator",
            "description": "Now we create the SwerveDrivePoseEstimator with all the required parameters. You'll need the kinematics you defined, initial sensor readings (gyro heading and module positions), a starting pose, and the standard deviation parameters. The estimator uses these to initialize its internal state.",
            "content": "import edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.kinematics.SwerveModulePosition;\n\n// Get initial sensor readings from hardware\nRotation2d initialHeading = m_gyro.getRotation2d();\nSwerveModulePosition[] initialModulePositions = getModulePositions();\nPose2d initialPose = new Pose2d();\n\n// Create pose estimator with all required parameters\nm_poseEstimator = new SwerveDrivePoseEstimator(\n    m_kinematics,\n    initialHeading,\n    initialModulePositions,\n    initialPose,\n    stateStdDevs,\n    visionStdDevs\n);"
        },
        {
            "type": "text",
            "title": "Updating with Odometry",
            "content": "Odometry updates happen continuously—every robot loop. This provides the fast, continuous tracking that keeps your pose estimate current even when vision isn't available."
        },
        {
            "type": "code",
            "title": "Getting Module Positions",
            "description": "First, we need to get the current position of each swerve module. Each module tracks its own position (distance traveled and angle) through its encoder. We collect these positions into an array that the pose estimator can use.",
            "content": "import edu.wpi.first.math.kinematics.SwerveModulePosition;\n\npublic void updateOdometry() {\n    // Read position from each swerve module\n    m_modulePositions[0] = m_modules[0].getPosition();\n    m_modulePositions[1] = m_modules[1].getPosition();\n    m_modulePositions[2] = m_modules[2].getPosition();\n    m_modulePositions[3] = m_modules[3].getPosition();\n}"
        },
        {
            "type": "code",
            "title": "Updating the Estimator",
            "description": "Now we update the pose estimator with the current gyro heading and module positions. The estimator uses these sensor readings to calculate how the robot has moved since the last update, incorporating this into its pose estimate.",
            "content": "import edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.kinematics.SwerveModulePosition;\n\npublic void updateOdometry() {\n    // Get current sensor readings\n    Rotation2d heading = m_gyro.getRotation2d();\n    \n    // Get module positions\n    SwerveModulePosition[] modulePositions = getModulePositions();\n    \n    // Update pose estimator\n    m_poseEstimator.update(heading, modulePositions);\n}"
        },
        {
            "type": "text",
            "title": "Adding Vision Measurements",
            "content": "Vision updates are asynchronous—they arrive when tags are detected, not on a fixed schedule. The pose estimator handles this automatically, incorporating vision measurements whenever they're available to correct any drift that has accumulated.<br><br>It's important to validate vision measurements before adding them to the pose estimator. Reject measurements with high ambiguity, measurements from tags that are too far away, or measurements that don't have enough tags detected. This prevents bad measurements from corrupting your pose estimate."
        },
        {
            "type": "code",
            "title": "Integrating Vision with Pose Estimator",
            "description": "Adding vision measurements to your pose estimator. Both PhotonVision and Limelight provide pose estimates with timestamps for proper latency compensation.",
            "content": "import edu.wpi.first.math.VecBuilder;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport frc.robot.LimelightHelpers.PoseEstimate;\nimport java.util.Optional;\n\npublic void updateVision() {\n    // Get pose estimate from vision subsystem\n    Optional<PoseEstimate> visionPoseEstimate = m_vision.getAprilTagPoseEstimate();\n    \n    if (visionPoseEstimate.isPresent()) {\n        PoseEstimate estimate = visionPoseEstimate.get();\n        \n        // Validate measurement quality\n        boolean shouldReject = false;\n        \n        // Reject single-tag measurements with high ambiguity\n        if (estimate.tagCount == 1 && estimate.rawFiducials.length == 1) {\n            if (estimate.rawFiducials[0].ambiguity > 0.7) {\n                shouldReject = true;\n            }\n        }\n        \n        // Reject tags that are too far away\n        if (estimate.tagCount == 1 && estimate.rawFiducials.length == 1) {\n            if (estimate.rawFiducials[0].distToCamera > 3.0) {\n                shouldReject = true;\n            }\n        }\n        \n        // Reject if no tags detected\n        if (estimate.tagCount == 0) {\n            shouldReject = true;\n        }\n        \n        if (!shouldReject) {\n            // Adjust uncertainty based on tag count\n            // More tags = more accurate = lower uncertainty\n            if (estimate.tagCount >= 2) {\n                m_poseEstimator.setVisionMeasurementStdDevs(\n                    VecBuilder.fill(0.3, 0.3, 9999999)\n                );\n            } else {\n                m_poseEstimator.setVisionMeasurementStdDevs(\n                    VecBuilder.fill(0.5, 0.5, 9999999)\n                );\n            }\n            \n            // Add vision measurement with timestamp\n            m_poseEstimator.addVisionMeasurement(\n                estimate.pose,\n                estimate.timestampSeconds\n            );\n        }\n    }\n}"
        },
        {
            "type": "text",
            "title": "Putting It All Together",
            "content": "In your subsystem's periodic method, you'll update both odometry and vision. The pose estimator automatically handles the fusion, giving you the best possible pose estimate at any moment."
        },
        {
            "type": "code",
            "title": "Complete Periodic Update",
            "description": "This shows the periodic update flow that combines odometry and vision updates.",
            "content": "import edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\n\n@Override\npublic void periodic() {\n    // Update odometry continuously (always available)\n    updateOdometry();\n    \n    // Update vision when available (asynchronous)\n    updateVision();\n    \n    // Get the fused pose estimate\n    Pose2d currentPose = m_poseEstimator.getEstimatedPosition();\n    \n    // Optional: Display on SmartDashboard for debugging\n    SmartDashboard.putNumber(\"Fused X\", currentPose.getX());\n    SmartDashboard.putNumber(\"Fused Y\", currentPose.getY());\n    SmartDashboard.putNumber(\"Fused Heading\", \n        currentPose.getRotation().getDegrees());\n}"
        },
        {
            "type": "text",
            "title": "Calibration: The Foundation of Accuracy",
            "content": "Even the best fusion algorithm can't overcome poor sensor calibration. Your pose estimation is only as good as your sensor measurements. Calibration ensures your sensors report accurate values, which directly translates to accurate pose estimation.<br><br>Think of calibration as teaching your robot to measure correctly. If your encoders think the wheels are a different size than they actually are, every distance calculation will be wrong. If your gyro has an offset, every heading will be wrong. These errors compound over time, making accurate pose estimation impossible."
        },
        {
            "type": "text",
            "title": "Calibrating Wheel Diameters",
            "content": "Wheel diameter directly affects distance calculations. A small error in diameter measurement creates a systematic error in every distance measurement. Use calipers to measure the actual wheel diameter precisely—don't rely on manufacturer specifications, as wheels can compress or wear over time."
        },
        {
            "type": "code",
            "title": "Configuring Encoder Distance Per Pulse",
            "description": "Once you have the actual wheel diameter, calculate the distance per encoder pulse. This tells the encoder how far the robot moves for each encoder count. The formula is: distance per pulse = (wheel diameter × π) / encoder counts per revolution.",
            "content": "import edu.wpi.first.wpilibj.Encoder;\n\n// Measured values (calibrate these with actual hardware)\nprivate static final double kMeasuredWheelDiameter = 0.1524;\nprivate static final double kEncoderCPR = 2048.0;\n\n// Calculate distance per encoder pulse\nprivate static final double kDistancePerPulse = \n    (kMeasuredWheelDiameter * Math.PI) / kEncoderCPR;\n\n// Configure encoder with calibrated value\nm_encoder.setDistancePerPulse(kDistancePerPulse);"
        },
        {
            "type": "text",
            "title": "Calibrating Track Width",
            "content": "Track width (the distance between left and right wheels) affects rotation calculations. For swerve drives, you need both track width and wheelbase. Measure these carefully using precise tools—even a small error causes significant heading errors over time."
        },
        {
            "type": "code",
            "title": "Testing Calibration Accuracy",
            "description": "Drive a known distance and compare the encoder reading to verify your calibration. This test helps you identify systematic errors in your encoder configuration. If the error is significant, you'll need to adjust your wheel diameter measurement and recalculate the distance per pulse.",
            "content": "import edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\n\npublic void testCalibration(double knownDistanceMeters) {\n    double encoderDistance = m_encoder.getDistance();\n    double error = Math.abs(encoderDistance - knownDistanceMeters);\n    double errorPercent = (error / knownDistanceMeters) * 100.0;\n    \n    // Display results for analysis\n    SmartDashboard.putNumber(\"Calibration Error (m)\", error);\n    SmartDashboard.putNumber(\"Calibration Error (%)\", errorPercent);\n}"
        },
        {
            "type": "text",
            "title": "Vision System Calibration",
            "content": "Vision systems require camera calibration to correct for lens distortion and measure accurate distances. This is typically done using calibration tools provided by your vision library (PhotonVision, Limelight, etc.). The calibration process captures images of a known pattern and calculates the camera's intrinsic parameters."
        },
        {
            "type": "code",
            "title": "Validating Vision Measurements",
            "description": "Test vision accuracy by comparing vision estimates to known robot positions. Place your robot at a known field position (using a tape measure or field markings) and compare the vision pose estimate to the actual position. This helps identify calibration issues or camera mounting problems.",
            "content": "import edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\nimport java.util.Optional;\n\npublic void testVisionAccuracy(Pose2d knownActualPose) {\n    Optional<Pose2d> visionPose = m_vision.getAprilTagPose();\n    \n    if (visionPose.isPresent()) {\n        Pose2d estimatedPose = visionPose.get();\n        \n        // Calculate errors\n        double xError = Math.abs(estimatedPose.getX() - knownActualPose.getX());\n        double yError = Math.abs(estimatedPose.getY() - knownActualPose.getY());\n        double headingError = Math.abs(\n            estimatedPose.getRotation().minus(knownActualPose.getRotation()).getDegrees()\n        );\n        \n        // Display for analysis\n        SmartDashboard.putNumber(\"Vision X Error (m)\", xError);\n        SmartDashboard.putNumber(\"Vision Y Error (m)\", yError);\n        SmartDashboard.putNumber(\"Vision Heading Error (deg)\", headingError);\n    }\n}"
        },
        {
            "type": "text",
            "title": "Tuning Fusion Parameters",
            "content": "The standard deviation parameters in your PoseEstimator control how much weight each measurement type receives. These are tuning parameters that you adjust based on your sensor accuracy and field conditions."
        },
        {
            "type": "code",
            "title": "Adjusting Standard Deviations",
            "description": "Lower standard deviations mean more trust. If your odometry is very accurate (good encoders, well calibrated), use lower state standard deviations. If your vision is very accurate (good camera, multiple tags detected), use lower vision standard deviations. If sensors are less accurate, increase these values to reduce their influence.",
            "content": "import edu.wpi.first.math.VecBuilder;\n\n// If odometry is accurate (good encoders, calibrated well)\nvar stateStdDevs = VecBuilder.fill(0.05, 0.05, 0.05);\n\n// If vision is accurate (good camera, multiple tags)\nvar visionStdDevs = VecBuilder.fill(0.3, 0.3, 0.3);"
        },
        {
            "type": "text",
            "title": "Testing and Validation",
            "content": "Regular testing is essential to ensure your pose estimation remains accurate. Test at multiple field positions, under different conditions, and monitor for drift or errors. Use SmartDashboard to visualize your pose estimate and compare it to known positions."
        },
        {
            "type": "code",
            "title": "Pose Accuracy Testing",
            "description": "Test pose accuracy by driving to known positions and comparing estimates. This comprehensive test measures both position and heading errors, giving you a complete picture of your pose estimation accuracy. Use this to validate your calibration and tuning.",
            "content": "import edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\n\npublic void testPoseAccuracy(Pose2d targetPose, Pose2d actualPose) {\n    Pose2d estimatedPose = m_poseEstimation.getPose();\n    \n    // Calculate errors\n    double xError = Math.abs(estimatedPose.getX() - actualPose.getX());\n    double yError = Math.abs(estimatedPose.getY() - actualPose.getY());\n    double headingError = Math.abs(\n        estimatedPose.getRotation().minus(actualPose.getRotation()).getDegrees()\n    );\n    double positionError = estimatedPose.getTranslation()\n        .getDistance(actualPose.getTranslation());\n    \n    // Display for analysis\n    SmartDashboard.putNumber(\"Pose X Error (m)\", xError);\n    SmartDashboard.putNumber(\"Pose Y Error (m)\", yError);\n    SmartDashboard.putNumber(\"Pose Heading Error (deg)\", headingError);\n    SmartDashboard.putNumber(\"Pose Position Error (m)\", positionError);\n}"
        },
        {
            "type": "code",
            "title": "Complete Fused Pose Estimation Flow",
            "description": "This shows the complete flow from initialization through periodic updates, combining odometry and vision measurements.",
            "content": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Rotation2d;\nimport edu.wpi.first.math.geometry.Translation2d;\nimport edu.wpi.first.math.kinematics.SwerveDriveKinematics;\nimport edu.wpi.first.math.estimator.SwerveDrivePoseEstimator;\nimport edu.wpi.first.math.kinematics.SwerveModulePosition;\nimport edu.wpi.first.math.VecBuilder;\nimport edu.wpi.first.wpilibj.ADXRS450_Gyro;\nimport edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport frc.robot.LimelightHelpers;\nimport frc.robot.LimelightHelpers.PoseEstimate;\nimport java.util.Optional;\n\npublic class FusedPoseEstimation extends SubsystemBase {\n    private final SwerveDriveKinematics m_kinematics;\n    private SwerveDrivePoseEstimator m_poseEstimator;\n    private SwerveModulePosition[] m_modulePositions;\n    private final ADXRS450_Gyro m_gyro;\n    private SwerveModule[] m_modules;\n    private final VisionSubsystem m_vision;\n    \n    private static final double kTrackWidth = 0.5;\n    private static final double kWheelBase = 0.5;\n    \n    public FusedPoseEstimation(SwerveModule[] modules) {\n        Translation2d frontLeft = new Translation2d(kWheelBase / 2.0, kTrackWidth / 2.0);\n        Translation2d frontRight = new Translation2d(kWheelBase / 2.0, -kTrackWidth / 2.0);\n        Translation2d rearLeft = new Translation2d(-kWheelBase / 2.0, kTrackWidth / 2.0);\n        Translation2d rearRight = new Translation2d(-kWheelBase / 2.0, -kTrackWidth / 2.0);\n        \n        m_kinematics = new SwerveDriveKinematics(\n            frontLeft, frontRight, rearLeft, rearRight\n        );\n        \n        m_gyro = new ADXRS450_Gyro();\n        m_gyro.calibrate();\n        \n        m_modules = modules;\n        m_modulePositions = new SwerveModulePosition[4];\n        for (int i = 0; i < 4; i++) {\n            m_modulePositions[i] = m_modules[i].getPosition();\n        }\n        \n        var stateStdDevs = VecBuilder.fill(0.1, 0.1, 0.1);\n        var visionStdDevs = VecBuilder.fill(0.5, 0.5, 0.5);\n        \n        m_poseEstimator = new SwerveDrivePoseEstimator(\n            m_kinematics,\n            m_gyro.getRotation2d(),\n            m_modulePositions,\n            new Pose2d(),\n            stateStdDevs,\n            visionStdDevs\n        );\n        \n        m_vision = new VisionSubsystem();\n    }\n    \n    @Override\n    public void periodic() {\n        // Update odometry continuously\n        updateOdometry();\n        \n        // Update vision when available\n        updateVision();\n        \n        // Get fused pose estimate\n        Pose2d currentPose = getPose();\n        \n        // Optional: Display for debugging\n        SmartDashboard.putNumber(\"Fused X\", currentPose.getX());\n        SmartDashboard.putNumber(\"Fused Y\", currentPose.getY());\n        SmartDashboard.putNumber(\"Fused Heading\", \n            currentPose.getRotation().getDegrees());\n    }\n    \n    public void updateOdometry() {\n        Rotation2d heading = m_gyro.getRotation2d();\n        \n        m_modulePositions[0] = m_modules[0].getPosition();\n        m_modulePositions[1] = m_modules[1].getPosition();\n        m_modulePositions[2] = m_modules[2].getPosition();\n        m_modulePositions[3] = m_modules[3].getPosition();\n        \n        m_poseEstimator.update(heading, m_modulePositions);\n    }\n    \n    public void updateVision() {\n        Optional<PoseEstimate> visionEstimate = m_vision.getAprilTagPoseEstimate();\n        \n        if (visionEstimate.isPresent()) {\n            PoseEstimate estimate = visionEstimate.get();\n            \n            // Validate measurement quality\n            boolean isValid = estimate.tagCount > 0 && \n                LimelightHelpers.validPoseEstimate(estimate);\n            \n            if (isValid) {\n                if (estimate.tagCount >= 2) {\n                    m_poseEstimator.setVisionMeasurementStdDevs(\n                        VecBuilder.fill(0.3, 0.3, 9999999)\n                    );\n                } else {\n                    m_poseEstimator.setVisionMeasurementStdDevs(\n                        VecBuilder.fill(0.5, 0.5, 9999999)\n                    );\n                }\n                \n                m_poseEstimator.addVisionMeasurement(\n                    estimate.pose,\n                    estimate.timestampSeconds\n                );\n            }\n        }\n    }\n    \n    public Pose2d getPose() {\n        return m_poseEstimator.getEstimatedPosition();\n    }\n    \n    public void resetPose(Pose2d newPose) {\n        Rotation2d heading = m_gyro.getRotation2d();\n        SwerveModulePosition[] modulePositions = getModulePositions();\n        m_poseEstimator.resetPosition(heading, modulePositions, newPose);\n    }\n    \n    private SwerveModulePosition[] getModulePositions() {\n        SwerveModulePosition[] positions = new SwerveModulePosition[4];\n        for (int i = 0; i < 4; i++) {\n            positions[i] = m_modules[i].getPosition();\n        }\n        return positions;\n    }\n}"
        },
        {
            "type": "link-grid",
            "title": "Related Topics and Documentation",
            "links": [
                {
                    "label": "Pose Estimation Introduction",
                    "id": "pose-estimation-intro"
                },
                {
                    "label": "Odometry",
                    "id": "odometry"
                },
                {
                    "label": "Vision Subsystem",
                    "id": "vision-subsystem"
                },
                {
                    "label": "WPILib Pose Estimator",
                    "url": "https://docs.wpilib.org/en/stable/docs/software/advanced-controls/state-space/state-space-pose-estimators.html#pose-estimators"
                },
                {
                    "label": "WPILib Kinematics and Odometry",
                    "url": "https://docs.wpilib.org/en/stable/docs/software/kinematics-and-odometry/index.html"
                }
            ]
        }
    ]
}