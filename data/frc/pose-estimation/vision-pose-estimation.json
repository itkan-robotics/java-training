{
  "title": "Vision and AprilTag Pose Estimation",
  "sections": [
    {
      "type": "text",
      "title": "Introduction to Vision Pose Estimation",
      "content": "Vision-based pose estimation uses cameras to observe field features and calculate the robot's pose from these observations. Unlike odometry, which accumulates error over time, vision provides absolute positioning - the robot knows exactly where it is relative to known field features, eliminating drift.<br><br>AprilTags are fiducial markers (visual markers with known patterns) used in FRC for robot localization. Each AprilTag has a unique ID and a known position on the FRC field. By detecting AprilTags with a camera, robots can calculate their exact pose on the field with high accuracy. AprilTags are part of the standard FRC field design, with tags placed at strategic locations around the field.<br><br>Learn more: <a href='https://docs.wpilib.org/en/stable/docs/software/vision-processing/apriltag/apriltag-intro.html' target='_blank'>WPILib: AprilTag Introduction</a>"
    },
    {
      "type": "text",
      "title": "How Vision Pose Works",
      "content": "Vision pose estimation follows this process:<br><br><strong>Camera Sees Field Features:</strong> The camera captures images of the field and detects features with known positions (AprilTags, field elements). AprilTags are square markers with a unique black and white pattern that encodes a unique ID. The tag's size is known, allowing the system to calculate distance and angle from the camera.<br><br><strong>Calculates Pose from Observations:</strong> By analyzing where features appear in the camera image, the system calculates the camera's position and orientation relative to those features. This involves detecting features, measuring positions in the image, calculating camera pose relative to features (using camera calibration and geometry), and converting camera pose to robot pose (accounting for camera mounting position).<br><br><strong>FRC Field AprilTags:</strong> Each FRC game specifies AprilTag positions in the field layout documentation. Tags are typically placed at strategic locations (scoring areas, corners, mid-field) with exact X, Y, Z coordinates and orientation. Tag IDs correspond to their field positions, allowing robots to calculate their pose by detecting tags."
    },
    {
      "type": "text",
      "title": "Camera Setup",
      "content": "Proper camera setup is essential for accurate vision pose estimation:<br><br><strong>Camera Requirements:</strong> Use cameras suitable for AprilTag detection (most USB and network cameras work). Consider resolution (higher = better detection at distance), frame rate (higher = more updates), and field of view (wider = see more tags).<br><br><strong>Camera Mounting:</strong> Mount camera securely to avoid vibration. Position camera to see AprilTags during robot operation. Consider height (higher = better tag visibility), angle (tilt to see tags at different heights), and position (avoid obstructions). Ensure camera doesn't move relative to robot (affects pose calculation).<br><br><strong>Camera Calibration:</strong> Calibrate camera to correct for lens distortion and measure camera parameters. Calibration provides camera matrix and distortion coefficients. Essential for accurate pose calculation. Vision libraries typically provide calibration tools."
    },
    {
      "type": "code-tabs",
      "title": "AprilTag Detection Setup",
      "content": "Setting up AprilTag detection with different vision solutions:",
      "tabs": [
        {
          "label": "PhotonVision",
          "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.math.geometry.Transform3d;\nimport org.photonvision.PhotonCamera;\nimport org.photonvision.targeting.PhotonPipelineResult;\nimport org.photonvision.targeting.PhotonTrackedTarget;\nimport java.util.Optional;\n\npublic class AprilTagVision {\n    private final PhotonCamera m_camera;\n    \n    // Camera position relative to robot center (in meters)\n    private static final Transform3d kRobotToCamera = new Transform3d(\n        0.3, 0.0, 0.2,  // X, Y, Z position (meters)\n        new edu.wpi.first.math.geometry.Rotation3d()  // Rotation\n    );\n    \n    public AprilTagVision() {\n        // Camera name must match the name configured in PhotonVision UI\n        m_camera = new PhotonCamera(\"MainCamera\");\n    }\n    \n    /**\n     * Get pose estimate from AprilTag detection\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        \n        if (result.hasTargets()) {\n            PhotonTrackedTarget bestTarget = result.getBestTarget();\n            \n            // Check if it's a valid AprilTag (fiducial ID >= 0)\n            if (bestTarget.getFiducialId() >= 0) {\n                var poseEstimate = result.getEstimatedPose();\n                \n                if (poseEstimate.isPresent()) {\n                    return Optional.of(poseEstimate.get().toPose2d());\n                }\n            }\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get detected AprilTag ID\n     */\n    public Optional<Integer> getDetectedTagId() {\n        PhotonPipelineResult result = m_camera.getLatestResult();\n        \n        if (result.hasTargets()) {\n            PhotonTrackedTarget bestTarget = result.getBestTarget();\n            int fiducialId = bestTarget.getFiducialId();\n            \n            if (fiducialId >= 0) {\n                return Optional.of(fiducialId);\n            }\n        }\n        \n        return Optional.empty();\n    }\n}"
        },
        {
          "label": "Limelight",
          "code": "package frc.robot.subsystems;\n\nimport frc.robot.LimelightHelpers;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport java.util.Optional;\n\npublic class AprilTagVision {\n    // Limelight camera name (use \"limelight\" for default)\n    private static final String LIMELIGHT_NAME = \"limelight\";\n    \n    public AprilTagVision() {\n        // Configure Limelight pipeline (0-9)\n        LimelightHelpers.setPipelineIndex(LIMELIGHT_NAME, 0);\n    }\n    \n    /**\n     * Get pose estimate from AprilTag detection\n     * Limelight provides robot pose in field coordinates\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        LimelightHelpers.LimelightResults results = LimelightHelpers.getLatestResults(LIMELIGHT_NAME);\n        \n        // Check if we have valid targets\n        if (results.targetingResults.targets_Fiducials.length > 0) {\n            Pose2d botPose = LimelightHelpers.getBotPose2d(LIMELIGHT_NAME);\n            \n            // Check if pose is valid (not all zeros)\n            if (botPose.getX() != 0.0 || botPose.getY() != 0.0) {\n                return Optional.of(botPose);\n            }\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get detected AprilTag ID\n     */\n    public Optional<Integer> getDetectedTagId() {\n        LimelightHelpers.LimelightResults results = LimelightHelpers.getLatestResults(LIMELIGHT_NAME);\n        \n        if (results.targetingResults.targets_Fiducials.length > 0) {\n            LimelightHelpers.LimelightTarget_Fiducial target = \n                results.targetingResults.targets_Fiducials[0];\n            \n            return Optional.of((int) target.fiducialID);\n        }\n        \n        return Optional.empty();\n    }\n    \n    /**\n     * Get number of detected tags (Limelight automatically fuses multiple tags)\n     */\n    public int getDetectedTagCount() {\n        LimelightHelpers.LimelightResults results = LimelightHelpers.getLatestResults(LIMELIGHT_NAME);\n        return results.targetingResults.targets_Fiducials.length;\n    }\n}"
        },
        {
          "label": "WPILib",
          "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.apriltag.AprilTagFieldLayout;\nimport edu.wpi.first.apriltag.AprilTagFields;\nimport edu.wpi.first.cameraserver.CameraServer;\nimport edu.wpi.first.cscore.UsbCamera;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport java.util.Optional;\n\npublic class AprilTagVision extends SubsystemBase {\n    private final UsbCamera m_camera;\n    private final AprilTagFieldLayout m_fieldLayout;\n    \n    public AprilTagVision() {\n        m_camera = CameraServer.startAutomaticCapture(0);\n        m_camera.setResolution(640, 480);\n        m_camera.setFPS(30);\n        \n        // Load field layout (contains AprilTag positions)\n        try {\n            m_fieldLayout = AprilTagFieldLayout.loadField(AprilTagFields.kDefaultField);\n        } catch (Exception e) {\n            System.err.println(\"Failed to load AprilTag field layout: \" + e.getMessage());\n            m_fieldLayout = null;\n        }\n        \n        // Note: WPILib's AprilTag support requires additional vision processing\n        // You would typically use PhotonVision or Limelight for actual detection\n    }\n    \n    /**\n     * Get pose estimate from AprilTag detection\n     * Note: This requires vision processing (PhotonVision, Limelight, or custom pipeline)\n     */\n    public Optional<Pose2d> getAprilTagPose() {\n        // For actual implementation, use PhotonVision or Limelight\n        return Optional.empty();\n    }\n}"
        }
      ]
    },
    {
      "type": "text",
      "title": "Multiple Tag Detection and Fusion",
      "content": "Using multiple tags improves accuracy and reliability:<br><br><strong>Better Accuracy:</strong> Detecting multiple AprilTags simultaneously provides more observations for pose calculation. More observations reduce error and improve accuracy. Vision systems can average or weight multiple tag observations.<br><br><strong>Redundancy:</strong> Multiple tags provide redundancy - if one tag is temporarily blocked, others may still be visible. This improves reliability and continuous pose estimation.<br><br><strong>Automatic Fusion:</strong> Vision systems like Limelight automatically fuse multiple tag observations when available. PhotonVision can also combine observations from multiple tags. This typically involves averaging poses or using weighted combinations based on detection confidence or tag distance."
    },
    {
      "type": "code-tabs",
      "title": "Integrating Vision Pose with Odometry",
      "content": "Integrating AprilTag poses with odometry for sensor fusion:",
      "tabs": [
        {
          "label": "PhotonVision",
          "code": "package frc.robot.subsystems;\n\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.Timer;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport java.util.Optional;\n\npublic class VisionOdometryIntegration extends SubsystemBase {\n    private final AprilTagVision m_aprilTagVision;\n    private final DrivetrainOdometry m_odometry;\n    \n    public VisionOdometryIntegration(\n            AprilTagVision aprilTagVision,\n            DrivetrainOdometry odometry) {\n        m_aprilTagVision = aprilTagVision;\n        m_odometry = odometry;\n    }\n    \n    @Override\n    public void periodic() {\n        m_odometry.updateOdometry();\n        \n        Optional<Pose2d> aprilTagPose = m_aprilTagVision.getAprilTagPose();\n        \n        if (aprilTagPose.isPresent()) {\n            // Simple approach: Reset odometry to AprilTag pose (corrects drift)\n            m_odometry.resetOdometry(aprilTagPose.get());\n            \n            // For sensor fusion, use WPILib's PoseEstimator:\n            // double timestamp = Timer.getFPGATimestamp();\n            // m_poseEstimator.addVisionMeasurement(aprilTagPose.get(), timestamp);\n        }\n        \n        Pose2d currentPose = m_odometry.getPose();\n    }\n}"
        },
        {
          "label": "Limelight",
          "code": "package frc.robot.subsystems;\n\nimport frc.robot.LimelightHelpers;\nimport edu.wpi.first.math.geometry.Pose2d;\nimport edu.wpi.first.wpilibj.Timer;\nimport edu.wpi.first.wpilibj2.command.SubsystemBase;\nimport java.util.Optional;\n\npublic class VisionOdometryIntegration extends SubsystemBase {\n    private final AprilTagVision m_aprilTagVision;\n    private final DrivetrainOdometry m_odometry;\n    \n    public VisionOdometryIntegration(\n            AprilTagVision aprilTagVision,\n            DrivetrainOdometry odometry) {\n        m_aprilTagVision = aprilTagVision;\n        m_odometry = odometry;\n    }\n    \n    @Override\n    public void periodic() {\n        m_odometry.updateOdometry();\n        \n        Optional<Pose2d> aprilTagPose = m_aprilTagVision.getAprilTagPose();\n        \n        if (aprilTagPose.isPresent()) {\n            // Simple approach: Reset odometry to AprilTag pose\n            m_odometry.resetOdometry(aprilTagPose.get());\n            \n            // Alternative: Use Limelight's PoseEstimate with latency compensation\n            // LimelightHelpers.PoseEstimate poseEstimate = \n            //     LimelightHelpers.getBotPoseEstimate_wpiBlue(\"limelight\");\n            // if (poseEstimate.tagCount >= 2) {\n            //     m_poseEstimator.addVisionMeasurement(\n            //         poseEstimate.pose, \n            //         Timer.getFPGATimestamp() - (poseEstimate.latency / 1000.0)\n            //     );\n            // }\n        }\n        \n        Pose2d currentPose = m_odometry.getPose();\n    }\n}"
        }
      ]
    },
    {
      "type": "text",
      "title": "Vision Pose Characteristics",
      "content": "Understanding vision update characteristics and limitations:<br><br><strong>Update Rate:</strong> Vision systems typically update at 10-30 Hz (much slower than odometry's 50 Hz). Update rate depends on camera frame rate, processing time, and feature detection frequency. Updates are asynchronous - they arrive when features are detected, not every robot loop.<br><br><strong>Latency:</strong> Vision processing introduces latency (typically 20-100ms). This means vision pose estimates are slightly delayed compared to current robot position. For fast-moving robots, this latency must be accounted for. Sensor fusion algorithms can handle this latency by timestamping measurements.<br><br><strong>Limitations:</strong> Vision only works when field features are visible. If camera can't see AprilTags, no pose estimate is available. Obstacles, other robots, or camera positioning can block features. Vision performance also depends on lighting conditions - poor lighting can reduce detection reliability."
    },
    {
      "type": "link-grid",
      "title": "Related Topics and Documentation",
      "links": [
        {
          "label": "WPILib AprilTag Introduction",
          "url": "https://docs.wpilib.org/en/stable/docs/software/vision-processing/apriltag/apriltag-intro.html"
        },
        {
          "label": "PhotonVision Documentation",
          "url": "https://docs.photonvision.org/"
        },
        {
          "label": "Limelight Documentation",
          "url": "https://docs.limelightvision.io/docs/docs-limelight/getting-started/summary"
        },
        {
          "label": "Pose Estimation Sensor Fusion",
          "id": "pose-estimation-fusion"
        }
      ]
    }
  ]
}